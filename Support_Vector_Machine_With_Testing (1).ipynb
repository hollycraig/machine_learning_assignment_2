{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4CD5VkiV-jX0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# centre the data\n",
    "def centre_data(train, validation, test):\n",
    "    \n",
    "    # calculate the means for each attribute of the training data\n",
    "    column_means = np.mean(train, axis=0) \n",
    "    \n",
    "    # centre training data by subtracting training data attribute means\n",
    "    for i in range(len(train)):\n",
    "        train[i] = train[i] - column_means\n",
    "    \n",
    "    # centre testing data by subtracting training data attribute means\n",
    "    for x in range(len(test)):\n",
    "        test[x] = test[x] - column_means\n",
    "        \n",
    "    for x in range(len(validation)):\n",
    "        validation[x] = validation[x] - column_means\n",
    "        \n",
    "    return train, test, validation\n",
    "\n",
    "# apply PCA on the data \n",
    "def PCA(variance_target, training_data, validation_data, testing_data):\n",
    "\n",
    "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
    "    \n",
    "    sum_square_singular = np.sum(sigma**2)\n",
    "    \n",
    "    ratios = sigma**2/sum_square_singular\n",
    "    n_components = 0\n",
    "    explained_variance = 0\n",
    "    \n",
    "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
    "    for i in range(len(ratios)):\n",
    "        if explained_variance >= variance_target:\n",
    "            break\n",
    "        else: \n",
    "            n_components += 1\n",
    "            explained_variance += ratios[i]\n",
    "    \n",
    "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
    "\n",
    "\n",
    "# calculate gradient, use L2 regularisation \n",
    "def calculate_gradient(weights, regularisation_param, training_data, training_labels, num_classes):\n",
    "    \n",
    "    num_training_samples, num_training_features = training_data.shape\n",
    "    \n",
    "    gradient = np.zeros(num_training_features, num_classes)\n",
    "    \n",
    "    # w^Tx\n",
    "    linear_output = np.dot(training_data, weights)\n",
    "\n",
    "    #linear output with labels \n",
    "    linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
    "  # distance of point from separating hyperplane?\n",
    "    # np.newaxis here makes it a column vector \n",
    "    # calculate distance?\n",
    "    \n",
    "    # distances = y * (np.dot(X, w)) - 1\n",
    "    delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
    "    \n",
    "    ones_and_zeros = np.zeros(delta.shape)\n",
    "    \n",
    "    # makes all the places where delta > 0, 1 else 0\n",
    "    # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
    "    # else: ð›¼ = 0\n",
    "    ones_and_zeros = np.where(delta > 0, 1, 0)\n",
    "    \n",
    "    # calculate the sum of each row \n",
    "    sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
    "    \n",
    "    ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
    "\n",
    "    gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
    "    \n",
    "    # controls the influence of each individual support vector on the objective function. \n",
    "    # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
    "    gradient = gradient + (2* regularisation_param * weights)\n",
    "    \n",
    "    return gradient \n",
    "\n",
    "# train model using stochastic gradient descent \n",
    "def train_model(training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
    "  \n",
    "    # number of examples in each batch\n",
    "    #batch_size = 200\n",
    "    num_training_samples = len(training_data)\n",
    "    weights = weights\n",
    "    for i in range(iterations):\n",
    "        # create batch\n",
    "        batch = np.random.choice(num_training_samples, batch_size)\n",
    "        gradient = calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
    "        weights = weights - learning_rate * gradient\n",
    "    return weights\n",
    "\n",
    "# calculate accuracy of model \n",
    "def calculate_accuracy (data, labels, weights):\n",
    "    accuracy = 0\n",
    "\n",
    "    prediction = np.zeros(len(data))\n",
    "\n",
    "    #w^Tx\n",
    "    linear_output = np.dot(data, weights)\n",
    "\n",
    "    # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
    "    # column index corresponding to the greatest index of each row\n",
    "    prediction = np.argmax(linear_output, axis=1)\n",
    "\n",
    "    # count the number of predictions that are correct \n",
    "    total_correct_predictions = (prediction == labels).sum()\n",
    "    num_data_points = len(data)\n",
    "\n",
    "    accuracy = (total_correct_predictions/num_data_points)*100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "def increase_image_contrast(image):\n",
    "    xp = [0, 64, 128, 192, 255]\n",
    "    fp = [0, 16, 128, 240, 255]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def increase_all_contrast(train, test):\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = increase_image_contrast(train[i])\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = increase_image_contrast(test[i])\n",
    "    return new_train, new_test\n",
    "\n",
    "# helper function for concatenating labels onto their corresponding data points\n",
    "def concatenate_data(training_data, training_labels):\n",
    "    return np.column_stack((training_data, training_labels))\n",
    "\n",
    "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
    "def split_into_validation_training(training_matrix):\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(training_matrix)\n",
    "\n",
    "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
    "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
    "    \n",
    "    return training_set, validation_set\n",
    "\n",
    "def load_in_dataset_and_preprocess(explained_variance):\n",
    "  \n",
    "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "    \n",
    "    # reshape the data \n",
    "    training_data = training_data.reshape(50000, 3072)\n",
    "    testing_data = testing_data.reshape(10000, 3072)\n",
    "\n",
    "    concatenated_training = concatenate_data(training_data, training_labels)\n",
    "\n",
    "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
    "\n",
    "    training_data = training_set[:, :-1]\n",
    "    training_labels = np.squeeze(training_set[:, -1])\n",
    "\n",
    "    validation_data = validation_set[:, :-1]\n",
    "    validation_labels = np.squeeze(validation_set[:, -1])\n",
    "\n",
    "    training_data = training_data.astype('float32')\n",
    "    testing_data = testing_data.astype('float32')\n",
    "    validation_data = validation_data.astype('float32')\n",
    "\n",
    "    # Centre data\n",
    "    #training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
    "\n",
    "    # Apply PCA\n",
    "    #training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
    "\n",
    "    number_training_samples = len(training_data)\n",
    "    number_validation_samples = len(validation_data)\n",
    "    number_testing_samples = len(testing_data)\n",
    "\n",
    "    # Reshape data from channel to rows\n",
    "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
    "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
    "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
    "\n",
    "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "ilJLBvE6g_Jr",
    "outputId": "97609f74-daee-4c6c-b88b-7874a42c3c6c"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'SVM' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a6f6a1ae6d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0000001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a6f6a1ae6d72>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularisation_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mnum_training_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'SVM' has no len()"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "class SVM:\n",
    "\n",
    "    # calculate gradient, use L2 regularisation \n",
    "    def calculate_gradient(weights, regularisation_param, training_data, training_labels, num_classes):\n",
    "      \n",
    "        num_training_samples, num_training_features = training_data.shape\n",
    "      \n",
    "        gradient = np.zeros(num_training_features, num_classes)\n",
    "\n",
    "        # w^Tx\n",
    "        linear_output = calculate_linear_output(training_data, weights)\n",
    "\n",
    "        #linear output with labels \n",
    "        linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
    "        delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
    "\n",
    "        ones_and_zeros = np.zeros(delta.shape)\n",
    "\n",
    "        # makes all the places where delta > 0, 1 else 0\n",
    "        # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
    "        # else: ð›¼ = 0\n",
    "        ones_and_zeros = np.where(delta > 0, 1, 0)\n",
    "\n",
    "        # calculate the sum of each row \n",
    "        sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
    "\n",
    "        ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
    "\n",
    "        gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
    "\n",
    "        # controls the influence of each individual support vector on the objective function. \n",
    "        # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
    "        gradient = gradient + (2* regularisation_param * weights)\n",
    "\n",
    "        return gradient \n",
    "\n",
    "    # train model using stochastic gradient descent \n",
    "    def train_model(training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
    "    \n",
    "        num_training_samples = len(training_data)\n",
    "        weights = weights\n",
    "\n",
    "        for i in range(iterations):\n",
    "        # create batch\n",
    "            batch = np.random.choice(num_training_samples, batch_size)\n",
    "            gradient = calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
    "            weights = weights - learning_rate * gradient\n",
    "\n",
    "        return weights\n",
    "\n",
    "    # calculate accuracy of model \n",
    "    def calculate_accuracy (data, labels, weights):\n",
    "      \n",
    "        accuracy = 0\n",
    "        prediction = np.zeros(len(data))\n",
    "\n",
    "        #w^Tx\n",
    "        linear_output= calculate_linear_output(data, weights)\n",
    "\n",
    "        # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
    "        # column index corresponding to the greatest index of each row\n",
    "        prediction = np.argmax(linear_output, axis=1)\n",
    "\n",
    "        # count the number of predictions that are correct \n",
    "        total_correct_predictions = (prediction == labels).sum()\n",
    "        num_data_points = len(data)\n",
    "        accuracy = (total_correct_predictions/num_data_points)*100\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_linear_output(data, weights):\n",
    "        return np.dot(data, weights)\n",
    "\n",
    "    # distance of point from separating hyperplane?\n",
    "    def calculate_distance(X, w):\n",
    "        return  y * (np.dot(X, w)) - 1\n",
    "\n",
    "def load_in_dataset_and_preprocess(explained_variance):\n",
    "  \n",
    "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "\n",
    "    # reshape the data \n",
    "    training_data = training_data.reshape(50000, 3072)\n",
    "    testing_data = testing_data.reshape(10000, 3072)\n",
    "\n",
    "    concatenated_training = concatenate_data(training_data, training_labels)\n",
    "\n",
    "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
    "\n",
    "    training_data = training_set[:, :-1]\n",
    "    training_labels = np.squeeze(training_set[:, -1])\n",
    "\n",
    "    validation_data = validation_set[:, :-1]\n",
    "    validation_labels = np.squeeze(validation_set[:, -1])\n",
    "\n",
    "    training_data = training_data.astype('float32')\n",
    "    testing_data = testing_data.astype('float32')\n",
    "    validation_data = validation_data.astype('float32')\n",
    "\n",
    "    # Centre data\n",
    "    #training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
    "\n",
    "    # Apply PCA\n",
    "    #training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
    "\n",
    "    number_training_samples = len(training_data)\n",
    "    number_validation_samples = len(validation_data)\n",
    "    number_testing_samples = len(testing_data)\n",
    "\n",
    "    # Reshape data from channel to rows\n",
    "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
    "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
    "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
    "\n",
    "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n",
    "\n",
    "\n",
    "training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "svm = SVM()\n",
    "num_classes = np.max(training_labels) + 1\n",
    "weights = np.ones((len(training_data[0]), num_classes))\n",
    "svm.train_model(training_data, training_labels, weights, 0.0000001, 5000, 15000, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZZ7OH1ojs7A"
   },
   "source": [
    "**Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbbcZjoxjcCI"
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "import keras\n",
    "\n",
    "\n",
    "def convert_to_grayscale(train, test):\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    gray_train = np.dot(train[:][...,:3], rgb_weights)\n",
    "    gray_test = np.dot(test[:][...,:3], rgb_weights)\n",
    "    return gray_train.astype(int), gray_test.astype(int)\n",
    "\n",
    "def increase_image_contrast(image, strength):\n",
    "    p_low, p_high = np.percentile(image, (strength, 100 - strength))\n",
    "    return (exposure.rescale_intensity(image, in_range=(p_low, p_high))).astype(int)\n",
    "\n",
    "def increase_all_contrast(train, test, strength=10):\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = increase_image_contrast(train[i], strength)\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = increase_image_contrast(test[i], strength)\n",
    "    return new_train, new_test\n",
    "\n",
    "def equalize_hist(image):\n",
    "    return (exposure.equalize_hist(image) * 255).astype(int)\n",
    "\n",
    "def equalize_hist_all(train, test):\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = equalize_hist(train[i])\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = equalize_hist(test[i])\n",
    "    return new_train, new_test\n",
    "\n",
    "def equalize_adapthist(image, clip_lim):\n",
    "    return (exposure.equalize_adapthist(image, clip_limit=clip_lim) * 255).astype(int)\n",
    "\n",
    "def equalize_adapthist_all(train, test, clip_limit=0.03):\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = equalize_adapthist(train[i], clip_limit)\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = equalize_adapthist(test[i], clip_limit)\n",
    "    return new_train, new_test\n",
    "\n",
    "def canny_edge_filter_all(train, test, sig=1):\n",
    "    if (len(train.shape) > 3):\n",
    "        train, test = convert_to_grayscale(train, test)\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = feature.canny(train[i].astype(float), sigma=sig)\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = feature.canny(test[i].astype(float), sigma=sig)\n",
    "    return new_train, new_test\n",
    "\n",
    "def autoencoder(training_data, testing_data):\n",
    "\n",
    "    from keras import layers\n",
    "\n",
    "    dimension = 32 \n",
    "    input_image = keras.Input(shape=(3072,)) #3072\n",
    "\n",
    "    encoded = layers.Dense(dimension, activation='relu')(input_image)\n",
    "    decoded = layers.Dense(3072, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = keras.Model(input_image, decoded)\n",
    "    encoder = keras.Model(input_image, encoded)\n",
    "    input_encode = keras.Input(shape=(dimension,))\n",
    "    layers = autoencoder.layers[-1]\n",
    "\n",
    "    decoder = keras.Model(input_encode, layers(input_encode))\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    autoencoder.fit(training_data, training_data,\n",
    "                  epochs=50,\n",
    "                  batch_size=256,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(testing_data, testing_data))\n",
    "\n",
    "    test_encoded = encoder.predict(testing_data)\n",
    "    train_encoded = encoder.predict(training_data)\n",
    "    decoded_test = decoder.predict(test_encoded)\n",
    "    decoded_train= decoder.predict(train_encoded)\n",
    "\n",
    "    return decoded_train, decoded_test\n",
    "\n",
    "\n",
    "def flatten_data(x_train, y_train, x_test, y_test):\n",
    "    new_image_shape = 1\n",
    "    for dim in range(1, len(x_train.shape)):\n",
    "        new_image_shape *= x_train.shape[dim]\n",
    "        \n",
    "    flat_x_train = x_train.reshape((x_train.shape[0], new_image_shape))\n",
    "    flat_y_train = np.ravel(y_train)\n",
    "    \n",
    "    flat_x_test = x_test.reshape((x_test.shape[0], new_image_shape))\n",
    "    flat_y_test = np.ravel(y_test)\n",
    "    return flat_x_train, flat_y_train, flat_x_test, flat_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRsMcwbh_HEe"
   },
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRjlzchTOzRO"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8ZcTXNp_N-Y"
   },
   "source": [
    "**Testing weight parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETX6HfcGiW5T"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def nano_to_seconds(nanoseconds):\n",
    "    \"\"\"Converts nanoseconds to seconds rounded to the nearest 5 decimal places.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nanoseconds : int\n",
    "        The nanoseconds to convert\n",
    "    \"\"\"\n",
    "\n",
    "    return np.round((nanoseconds / 1e+9), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-JeOYo9CZZa",
    "outputId": "ac2e35e6-424c-4ff1-e5a8-00cf48b3d2da"
   },
   "outputs": [],
   "source": [
    "def test_svm_weights():\n",
    "\n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "\n",
    "    num_classes = np.max(training_labels) + 1\n",
    "    weights1 = np.ones((len(training_data[0]), num_classes))\n",
    "    weights2 = np.ones((len(training_data[1]), num_classes))\n",
    "    weights3 = np.ones((len(training_data[2]), num_classes))\n",
    "\n",
    "    weights_list = [weights1, weights2, weights3]\n",
    "\n",
    "    with open('svm_learning_rates.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        iterations= 15000\n",
    "        batch_size=200\n",
    "        learning_rate= 0.00000001\n",
    "\n",
    "        for n_weight in weights_list:\n",
    "            i=0\n",
    "            num_tests = 0\n",
    "            total_accuracy = 0\n",
    "            total_runtime = 0\n",
    "\n",
    "            for i in range(2):\n",
    "                start_time = time.time()\n",
    "                res_weights = train_model(training_data, training_labels, n_weight, learning_rate, reg_param, iterations,batch_size, num_classes)\n",
    "                total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                print(total_accuracy)\n",
    "                training_time = time.time() - startTime\n",
    "                total_runtime += nano_to_seconds(training_time)\n",
    "                num_tests += 1\n",
    "\n",
    "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "            result_writer.writerow([i, learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "            i += 1\n",
    "\n",
    "# test_svm_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28mpQxaC_YgJ"
   },
   "source": [
    "**Testing learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXxtdOoZ_fX6",
    "outputId": "6d7fba92-ce5a-468a-dccd-d614c758ff59"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "def test_svm_learning_rate():\n",
    "    \n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    \n",
    "    num_classes = np.max(training_labels) + 1\n",
    "\n",
    "    learning_rates = [0.00001, 0.00000001, 0.0000000001, 0.00000000001]\n",
    "    \n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_learning_rates.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        iterations= 15000\n",
    "        batch_size=200\n",
    "\n",
    "        for n_learning_rate in learning_rates:\n",
    "                num_tests = 0\n",
    "                total_accuracy = 0\n",
    "                total_runtime = 0\n",
    "\n",
    "                for i in range(2):\n",
    "                    start_time = time.time()\n",
    "                    res_weights = train_model(training_data, training_labels, weights, n_learning_rate, reg_param, iterations,batch_size, num_classes)\n",
    "                    total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                    print(total_accuracy)\n",
    "                    training_time = time.time() - startTime\n",
    "                    total_runtime += nano_to_seconds(training_time)\n",
    "                    num_tests += 1\n",
    "\n",
    "                avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "                avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "                result_writer.writerow(['Weight Set', n_learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYG50PBA_iZp"
   },
   "source": [
    "**Testing regularisation parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4QgGb7bEHF_",
    "outputId": "e2193489-9e93-484f-a856-c52e1a777407"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "def test_svm_reg_param():\n",
    "    \n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    \n",
    "    num_classes = np.max(training_labels) + 1\n",
    "\n",
    "    reg_params = [1000,5000,7000,9000]\n",
    "    \n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_reg_param.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        iterations= 15000\n",
    "        learning_rate= 0.00000001\n",
    "        batch_size=200\n",
    "\n",
    "        for n_reg_param in reg_params:\n",
    "                num_tests = 0\n",
    "                total_accuracy = 0\n",
    "                total_runtime = 0\n",
    "\n",
    "                for i in range(2):\n",
    "                    start_time = time.time()\n",
    "                    res_weights = train_model(training_data, training_labels, weights, learning_rate, n_reg_param, iterations,batch_size, num_classes)\n",
    "                    total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                    print(total_accuracy)\n",
    "                    training_time = time.time() - startTime\n",
    "                    total_runtime += nano_to_seconds(training_time)\n",
    "                    num_tests += 1\n",
    "\n",
    "                avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "                avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "                result_writer.writerow(['Weight Set', learning_rate, n_reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_reg_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqQJOjDB_rUv"
   },
   "source": [
    "**Testing iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBk2vEbGELlW",
    "outputId": "916e8df3-d878-44b4-b533-7fc78027f4ee"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "def test_svm_iterations():\n",
    "    \n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    \n",
    "    num_classes = np.max(training_labels) + 1\n",
    "\n",
    "    iterations= [15000, 20000, 30000, 40000]\n",
    "    \n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_iterations.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        batch_size=200\n",
    "        learning_rate= 0.00000001\n",
    "\n",
    "        for n_iteration in iterations:\n",
    "                num_tests = 0\n",
    "                total_accuracy = 0\n",
    "                total_runtime = 0\n",
    "\n",
    "                for i in range(2):\n",
    "                    start_time = time.time()\n",
    "                    res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, n_iteration,batch_size, num_classes)\n",
    "                    total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                    print(total_accuracy)\n",
    "                    training_time = time.time() - startTime\n",
    "                    total_runtime += nano_to_seconds(training_time)\n",
    "                    num_tests += 1\n",
    "\n",
    "                avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "                avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "                result_writer.writerow(['Weight Set', learning_rate, reg_param, n_iteration, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_iterations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DyixbQI_4lL"
   },
   "source": [
    "**Testing batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZECne9sEMY3"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "def test_svm_batch_size():\n",
    "    \n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    \n",
    "    num_classes = np.max(training_labels) + 1\n",
    "\n",
    "    batch_sizes= [200, 400, 600]\n",
    "    \n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_batch_size.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        iterations= 15000\n",
    "        learning_rate= 0.00000001\n",
    "\n",
    "        for n_batch_size in batch_sizes:\n",
    "                num_tests = 0\n",
    "                total_accuracy = 0\n",
    "                total_runtime = 0\n",
    "\n",
    "                for i in range(2):\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, iterations,n_batch_size, num_classes)\n",
    "                    total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                    print(total_accuracy)\n",
    "                    training_time = time.time() - startTime\n",
    "                    total_runtime += nano_to_seconds(training_time)\n",
    "                    num_tests += 1\n",
    "\n",
    "                avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "                avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "                result_writer.writerow(['Weight Set', learning_rate, reg_param, iterations, n_batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_batch_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EtlyWxBhV96"
   },
   "source": [
    "**Testing All Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmLJD9tLhZ_q"
   },
   "outputs": [],
   "source": [
    "def test_svm_preprocessing_functions():\n",
    "\n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    num_classes = np.max(training_labels) + 1\n",
    "    techniques = ['none', 'hist equalization','adaptive equalisation', 'autoencoder' ] \n",
    "\n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_preprocessing.csv', mode='w', newline='') as csv_file:\n",
    "      \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        iterations= 15000\n",
    "        learning_rate= 0.00000001\n",
    "        batch_size=200\n",
    "\n",
    "        for technique in techniques:\n",
    "\n",
    "            num_tests = 0\n",
    "            total_accuracy = 0\n",
    "            total_runtime = 0\n",
    "\n",
    "            if technique == 'grayscale':\n",
    "                training_data, validation_data = convert_to_grayscale(training_data, validation_data)\n",
    "            elif technique == 'hist equalization':\n",
    "                training_data, validation_data = equalize_hist_all(training_data, validation_data)\n",
    "            elif technique == 'adaptive equalization':\n",
    "                training_data, validation_data = equalize_adapthist_all(training_data, validation_data)\n",
    "            elif technique == 'canny':\n",
    "                training_data, validation_data = canny_edge_filter_all(training_data, validation_data, 0)\n",
    "            elif technique == 'autoencoder':\n",
    "                training_data, validation_data = autoencoder(training_data, validation_data)\n",
    "\n",
    "        for i in range(2):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, iterations, batch_size, num_classes)\n",
    "            total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "            print(total_accuracy)\n",
    "            training_time = time.time() - startTime\n",
    "            total_runtime += nano_to_seconds(training_time)\n",
    "            num_tests += 1\n",
    "\n",
    "        avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "        avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "        result_writer.writerow([weights, learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_preprocessing_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NwSurIS6WcU"
   },
   "source": [
    "**Testing PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU2vo7pC6VrF",
    "outputId": "0fb579c0-fa75-456f-bc80-4340a3ecc1c0"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "def test_svm_pca():\n",
    "  \n",
    "    explained_variances = [0.6, 0.7, 0.8, 0.9]  \n",
    "\n",
    "    with open('svm_pca.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        iterations= 15000\n",
    "        learning_rate= 0.00000001\n",
    "        batch_size=200\n",
    "        reg_param=5000\n",
    "\n",
    "        for n_variance in explained_variances:\n",
    "            num_tests = 0\n",
    "            total_accuracy = 0\n",
    "            total_runtime = 0\n",
    "\n",
    "            for i in range(2):\n",
    "                training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(n_variance)\n",
    "                num_classes = np.max(training_labels) + 1\n",
    "                weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "                start_time = time.time()\n",
    "                res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, iterations,batch_size, num_classes)\n",
    "                total_accuracy += calculate_accuracy(validation_data, validation_labels, res_weights)\n",
    "                print(total_accuracy)\n",
    "                training_time = time.time() - startTime\n",
    "                total_runtime += nano_to_seconds(training_time)\n",
    "                num_tests += 1\n",
    "\n",
    "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "            result_writer.writerow(['Weights Set', learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "# test_svm_pca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooPPJox3HR3R"
   },
   "source": [
    "**Graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "Y1A27Rg6HRYU",
    "outputId": "daf31904-4cbf-4b49-d42a-4bbcc6564b5e"
   },
   "outputs": [],
   "source": [
    "with open('svm_batch_size.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "              })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['batch_size'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(n_estimators_list, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Value of Batch Sizes to Accuracy')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 23.073', xy=(600, 23.07), xytext=(600, 22.5),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "7xvxkPX3SSNk",
    "outputId": "86c87340-65c4-49b4-f20b-2c43099df1f2"
   },
   "outputs": [],
   "source": [
    "with open('svm_pca.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "            })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "  variances= [0.6, 0.7, 0.8, 0.9] \n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['batch_size'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(variances, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Value of Variance for PCA to Accuracy')\n",
    "    plt.xlabel('Explained Variance')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 24.40', xy=(0.9, 24.4), xytext=(0.9, 24.1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "HtWiVvGNXZZr",
    "outputId": "8939e5cc-f2a6-403d-f5cd-fc4b56e994cf"
   },
   "outputs": [],
   "source": [
    "with open('svm_iterations.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "            })\n",
    "    line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['iterations'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(n_estimators_list, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Number of Iterations to Accuracy')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 22.65', xy=(1.0, 22.65), xytext=(1.0, 22.4),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "3aN7VKPUYrTz",
    "outputId": "d5f07f08-74b0-4596-e170-96e64f94a4c7"
   },
   "outputs": [],
   "source": [
    "with open('svm_reg_param.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "            })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['reg_param'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(n_estimators_list, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Number of Regularisation Parameters to Accuracy')\n",
    "    plt.xlabel('Number of Regularisation Parameters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 24.64', xy=(0.04, 24.64), xytext=(0.1, 24.37),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "UQPvUJWNYvvQ",
    "outputId": "6623fef8-2561-4c32-e861-4ea2eb5e893a"
   },
   "outputs": [],
   "source": [
    "with open('svm_learning_rates.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "              })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['learning_rate'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(n_estimators_list, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Value of Learning Rate to Accuracy')\n",
    "    plt.xlabel('Value of Learning Rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 19.74', xy=(1.0, 19.74), xytext=(0.9, 16),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dDVJbymYzpM"
   },
   "outputs": [],
   "source": [
    "with open('svm_weights.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "              'weights' : (int)row[0],\n",
    "              'learning_rate' : row[1],\n",
    "              'reg_param' : row[2],\n",
    "              'iterations' : row[3],\n",
    "              'batch_size' : float(row[4]),\n",
    "              'accuracy' : float(row[5]),\n",
    "              'runtime' : float(row[6])\n",
    "            })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['weights'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(variances, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Various Sets of Weights to Accuracy')\n",
    "    plt.xlabel('Set of Weights')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 24.40', xy=(0.9, 24.4), xytext=(0.9, 24.1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvb3jtS5Y125"
   },
   "outputs": [],
   "source": [
    "with open('svm_preprocessing.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    results = []\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            results.append({\n",
    "                  'weights' : row[0],\n",
    "                  'learning_rate' : row[1],\n",
    "                  'reg_param' : row[2],\n",
    "                  'iterations' : row[3],\n",
    "                  'batch_size' : float(row[4]),\n",
    "                  'accuracy' : float(row[5]),\n",
    "                  'runtime' : float(row[6])\n",
    "            })\n",
    "        line_count += 1\n",
    "\n",
    "    n_estimators_list = []\n",
    "    accuracies = []\n",
    "    runtimes = []\n",
    "\n",
    "    for result in results:\n",
    "        n_estimators_list.append(result['iterations'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        runtimes.append(result['runtime'])\n",
    "\n",
    "    plt.plot(variances, accuracies, 'ro')\n",
    "\n",
    "    plt.title('Preprocessing Techniques to Accuracy to Accuracy')\n",
    "    plt.xlabel('Preprocessing Techniques')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.annotate('Maximum Accuracy: 24.40', xy=(0.9, 24.4), xytext=(0.9, 24.1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=8))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Support_Vector_Machine_With_Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
