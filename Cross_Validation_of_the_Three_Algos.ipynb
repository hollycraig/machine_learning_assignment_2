{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross Validation of the Three Algos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTDgHAD1D3Kw"
      },
      "source": [
        "**Loading and Preprocessing - setting up for cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKatvt6gD80d"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# centre the data\n",
        "def centre_data(train, test):\n",
        "    \n",
        "    # calculate the means for each attribute of the training data\n",
        "    column_means = np.mean(train, axis=0) \n",
        "    \n",
        "    # centre training data by subtracting training data attribute means\n",
        "    for i in range(len(train)):\n",
        "        train[i] = train[i] - column_means\n",
        "    \n",
        "    # centre testing data by subtracting training data attribute means\n",
        "    for x in range(len(test)):\n",
        "        test[x] = test[x] - column_means\n",
        "        \n",
        "    return train, test\n",
        "\n",
        "# apply PCA on the data \n",
        "def PCA(variance_target, training_data, testing_data):\n",
        "\n",
        "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
        "    \n",
        "    sum_square_singular = np.sum(sigma**2)\n",
        "    \n",
        "    ratios = sigma**2/sum_square_singular\n",
        "    n_components = 0\n",
        "    explained_variance = 0\n",
        "    \n",
        "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
        "    for i in range(len(ratios)):\n",
        "        if explained_variance >= variance_target:\n",
        "            break\n",
        "        else: \n",
        "            n_components += 1\n",
        "            explained_variance += ratios[i]\n",
        "    \n",
        "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components])\n",
        "\n",
        "\n",
        "\n",
        "def load_in_dataset_and_preprocess(explained_variance, training_data, testing_data, training_labels,testing_labels):\n",
        "    \n",
        "\n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "\n",
        "    # Centre data\n",
        "    #training_data, testing_data= centre_data(training_data, testing_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    #training_data, testing_data = PCA(explained_variance, training_data, testing_data)\n",
        "\n",
        "    number_training_samples = len(training_data)\n",
        "    number_testing_samples = len(testing_data)\n",
        "\n",
        "    # Reshape data from channel to rows\n",
        "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "    return training_data, training_labels, testing_data, testing_labels"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72E1OmgD9XB"
      },
      "source": [
        "**Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUu4JuHSEAFQ",
        "outputId": "0e809f4a-d60b-4946-9993-b2ee6771fd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "class SVM:\n",
        "\n",
        "    training_data=[]\n",
        "    testing_data=[]\n",
        "\n",
        "    def __init__(self, training_data, testing_data):\n",
        "      self.training_data= training_data\n",
        "      self.testing_data= testing_data\n",
        "\n",
        "    def __len__(self, data):\n",
        "      return len(data)\n",
        "\n",
        "    def calculate_linear_output(self, data, weights):\n",
        "      return np.dot(data, weights)\n",
        "\n",
        "    # distance of point from separating hyperplane?\n",
        "    def calculate_distance(self, X, w):\n",
        "      return  y * (np.dot(X, w)) - 1\n",
        "\n",
        "    # calculate gradient, use L2 regularisation \n",
        "    def calculate_gradient(self, weights, regularisation_param, training_data, training_labels, num_classes):\n",
        "        \n",
        "        num_training_samples, num_training_features = training_data.shape\n",
        "        \n",
        "        gradient = np.zeros((num_training_features, num_classes))\n",
        "        \n",
        "        # w^Tx\n",
        "        linear_output = self.calculate_linear_output(training_data, weights)\n",
        "\n",
        "        #linear output with labels \n",
        "        linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
        "        delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
        "        \n",
        "        ones_and_zeros = np.zeros(delta.shape)\n",
        "        \n",
        "        # makes all the places where delta > 0, 1 else 0\n",
        "        # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
        "        # else: ð›¼ = 0\n",
        "        ones_and_zeros = np.where(delta > 0, 1, 0)\n",
        "        \n",
        "        # calculate the sum of each row \n",
        "        sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
        "        \n",
        "        ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
        "\n",
        "        gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
        "        \n",
        "        # controls the influence of each individual support vector on the objective function. \n",
        "        # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
        "        gradient = gradient + (2* regularisation_param * weights)\n",
        "        \n",
        "        return gradient \n",
        "\n",
        "    # train model using stochastic gradient descent \n",
        "    def train_model(self, training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
        "      \n",
        "      num_training_samples = len(training_data)\n",
        "      weights = weights\n",
        "\n",
        "      for i in range(iterations):\n",
        "      # create batch\n",
        "          batch = np.random.choice(5000, batch_size) #change this to num_training_samples \n",
        "          gradient = self.calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
        "          weights = weights - learning_rate * gradient\n",
        "\n",
        "      return weights\n",
        "\n",
        "    # calculate accuracy of model \n",
        "    def calculate_accuracy (self, data, labels, weights):\n",
        "        \n",
        "        accuracy = 0\n",
        "        prediction = np.zeros(len(data))\n",
        "\n",
        "      #w^Tx\n",
        "        linear_output= self.calculate_linear_output(data, weights)\n",
        "\n",
        "      # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
        "      # column index corresponding to the greatest index of each row\n",
        "        prediction = np.argmax(linear_output, axis=1)\n",
        "\n",
        "      # count the number of predictions that are correct \n",
        "        total_correct_predictions = (prediction == labels).sum()\n",
        "        num_data_points = len(data)\n",
        "        accuracy = (total_correct_predictions/num_data_points)*100\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "#using 10 fold cross validation here to evaluate the performance of SVM\n",
        "def cross_validation():\n",
        "\n",
        "  (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "  cv = KFold(n_splits=10)\n",
        "  i=0\n",
        "  j=0\n",
        "\n",
        "  for train, test in cv.split(training_data):\n",
        "    \n",
        "    for train_label, test_label in cv.split(training_data):\n",
        "\n",
        "      if(i!=j):\n",
        "        j+=1\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        training_data, training_labels, testing_data, testing_labels = load_in_dataset_and_preprocess(0.9, train, train_label, test, test_label)\n",
        "        svm = SVM(training_data, testing_data)\n",
        "        num_classes = np.max(training_labels) + 1\n",
        "        weights = np.ones((len(training_data[1]), num_classes))\n",
        "        weights= svm.train_model(training_data, training_labels, weights, 0.00000001, 1000, 20000, 200, 20)\n",
        "\n",
        "        total_accuracy = svm.calculate_accuracy(testing_data, testing_labels, weights)\n",
        "        print('accuracy: ', total_accuracy)\n",
        "        break\n",
        "    i+=1\n",
        "\n",
        "cross_validation()\n",
        "\n",
        "  #run the classifiers here "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-3b2ef16cc90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0;31m#run the classifiers here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-3b2ef16cc90e>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00000001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-3b2ef16cc90e>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(self, data, labels, weights)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;31m# count the number of predictions that are correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtotal_correct_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mnum_data_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_correct_predictions\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_data_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYYhcv6V2OYU"
      },
      "source": [
        " for train, test in zip(cv.split(training_data), cv.split(training_labels)):\n",
        "   \n",
        "    training_data, training_labels, testing_data, testing_labels = load_in_dataset_and_preprocess(0.9, train[0], test[0], train[1], test[1])\n",
        "\n",
        "    svm = SVM(training_data, testing_data)\n",
        "    num_classes = np.max(training_labels) + 1\n",
        "    weights = np.ones((len(training_data[1]), num_classes))\n",
        "    weights= svm.train_model(training_data, training_labels, weights, 0.00000001, 1000, 20000, 200, 20)\n",
        "\n",
        "    total_accuracy = svm.calculate_accuracy(testing_data, testing_labels, weights)\n",
        "    print('accuracy: ', total_accuracy)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6hAGTTEAY5"
      },
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU525n67EC6W",
        "outputId": "0106f191-70ab-48d0-a7e2-2d97e715d502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils.np_utils import to_categorical  \n",
        "from keras.datasets import cifar100\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import numpy as np\n",
        "\n",
        "def run_MLP_model(training_data, training_labels, testing_data, testing_labels, first_activation_function, second_activation_function, num_hidden_units, learning_rate, optimiser, decay_level, momentum, epochs, loss_function):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(num_hidden_units, activation=first_activation_function, input_dim=training_data.shape[1]))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_hidden_units, activation=first_activation_function))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(20, activation=second_activation_function))\n",
        "\n",
        "    if (optimiser == 'SGD'):\n",
        "        op = SGD(lr=learning_rate, decay=decay_level, momentum=momentum, nesterov=True)\n",
        "\n",
        "    else:\n",
        "        op = Adam(lr=learning_rate, decay=decay_level)\n",
        "\n",
        "    # can also use loss function categorical_crossentropy\n",
        "    # or optimiser SGD\n",
        "    # try with different optimisers and loss functions\n",
        "    model.compile(optimizer=op,\n",
        "                  loss=loss_function,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(training_data, training_labels, epochs=epochs, batch_size=32, verbose=0, validation_split=0.2)\n",
        "\n",
        "    score = model.evaluate(testing_data, testing_labels, batch_size=128, verbose=0)\n",
        "    return score[1]\n",
        "\n",
        "# helper function for concatenating labels onto their corresponding data points\n",
        "def concatenate_data(training_data, training_labels):\n",
        "    return np.column_stack((training_data, training_labels))\n",
        "\n",
        "def cross_validation():\n",
        "\n",
        "  (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "  cv = KFold(n_splits=10)\n",
        "  momentum = 0.9\n",
        "  decay=1e-6\n",
        "  learning_rate = 0.001\n",
        "  first_activation_function = 'relu'\n",
        "  second_activation_function = 'softmax'\n",
        "  loss = 'sparse_categorical_crossentropy'\n",
        "  optimiser = 'Adam'\n",
        "  epochs = 20\n",
        "  num_hidden_units = 256\n",
        "\n",
        "  i=0\n",
        "  j=0\n",
        "\n",
        "  for train, test in cv.split(training_data):\n",
        "    \n",
        "    for train_label, test_label in cv.split(training_data):\n",
        "\n",
        "      if(i!=j):\n",
        "        j+=1\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        training_data, training_labels, testing_data, testing_labels = load_in_dataset_and_preprocess(0.9, train, train_label, test, test_label)\n",
        "        run_MLP_model(training_data, training_labels, testing_data, testing_labels, first_activation_function, second_activation_function, num_hidden_units, learning_rate, optimiser, decay, momentum, epochs, loss)\n",
        "\n",
        "        break\n",
        "    i+=1\n",
        "\n",
        "cross_validation()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3fb53efb6245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-3fb53efb6245>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_in_dataset_and_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mrun_MLP_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_activation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_activation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-3fb53efb6245>\u001b[0m in \u001b[0;36mrun_MLP_model\u001b[0;34m(training_data, training_labels, testing_data, testing_labels, first_activation_function, second_activation_function, num_hidden_units, learning_rate, optimiser, decay_level, momentum, epochs, loss_function)\u001b[0m\n\u001b[1;32m     34\u001b[0m                   metrics=['accuracy'])\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 36000\n  y sizes: 5000\nPlease provide data which shares the same first dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52gJ6_roEDSF"
      },
      "source": [
        "**Random Forests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGlmq6J8EFMs"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from skimage import exposure\n",
        "from skimage import feature\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def import_data() -> ((np.ndarray, np.ndarray), (np.ndarray, np.ndarray)):\n",
        "    return tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "# (x_train, y_train), (x_test, y_test) = import_data()\n",
        "\n",
        "# x_train, x_test = convert_to_grayscale(x_train, x_test)\n",
        "\n",
        "# x_train, x_test = increase_all_contrast(x_train, x_test, 10)\n",
        "\n",
        "# x_train, x_test = equalize_hist_all(x_train, x_test)\n",
        "\n",
        "# x_train, x_test = equalize_adapthist_all(x_train, x_test, 0.03)\n",
        "\n",
        "# x_train, x_test = canny_edge_filter_all(x_train, x_test, 0)\n",
        "\n",
        "\n",
        "# x_train, x_test = centre_data(x_train, x_test)\n",
        "\n",
        "# x_train, x_test = pca(x_train, x_test, 0.8)\n",
        "\n",
        "\n",
        "def cross_validation():\n",
        "\n",
        "  (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "  cv = KFold(n_splits=10)\n",
        "  momentum = 0.9\n",
        "  decay=1e-6\n",
        "  learning_rate = 0.001\n",
        "  first_activation_function = 'relu'\n",
        "  second_activation_function = 'softmax'\n",
        "  loss = 'sparse_categorical_crossentropy'\n",
        "  optimiser = 'Adam'\n",
        "  epochs = 20\n",
        "  num_hidden_units = 256\n",
        "\n",
        "  for train, test in zip(cv.split(training_data), cv.split(training_labels)):\n",
        "   \n",
        "    training_data, training_labels, testing_data, testing_labels = load_in_dataset_and_preprocess(0.8, train[0], train[1], test[0], test[1])\n",
        "    x_train, y_train, x_test, y_test = flatten_data(train[0], test[0], train[1], test[1])\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "    n_jobs=-1, \n",
        "    verbose=1,\n",
        "    n_estimators=100,\n",
        "    bootstrap=False, \n",
        "    max_features='log2', \n",
        "    criterion='gini')\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    model.score(x_test, y_test)\n",
        "\n",
        "cross_validation()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}