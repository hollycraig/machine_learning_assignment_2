{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP- Autoencoder and Image Whitening.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqJ5TRJn6qhn"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.np_utils import to_categorical  \n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# centre the data\n",
        "def centre_data(train, validation, test):\n",
        "    \n",
        "    # calculate the means for each attribute of the training data\n",
        "    column_means = np.mean(train, axis=0) \n",
        "    \n",
        "    # centre training data by subtracting training data attribute means\n",
        "    for i in range(len(train)):\n",
        "        train[i] = train[i] - column_means\n",
        "    \n",
        "    # centre testing data by subtracting training data attribute means\n",
        "    for x in range(len(test)):\n",
        "        test[x] = test[x] - column_means\n",
        "        \n",
        "    for x in range(len(validation)):\n",
        "        validation[x] = validation[x] - column_means\n",
        "        \n",
        "    return train, test, validation\n",
        "\n",
        "\n",
        "def autoencoder(training_data, testing_data):\n",
        "\n",
        "  from keras import layers\n",
        "\n",
        "  dimension = 32 \n",
        "  input_image = keras.Input(shape=(3072,))\n",
        "\n",
        "  encoded = layers.Dense(dimension, activation='relu')(input_image)\n",
        "  decoded = layers.Dense(3072, activation='sigmoid')(encoded)\n",
        "\n",
        "  autoencoder = keras.Model(input_image, decoded)\n",
        "  encoder = keras.Model(input_image, encoded)\n",
        "  input_encode = keras.Input(shape=(dimension,))\n",
        "  layers = autoencoder.layers[-1]\n",
        "\n",
        "  decoder = keras.Model(input_encode, layers(input_encode))\n",
        "  autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "  autoencoder.fit(training_data, training_data,\n",
        "                  epochs=50,\n",
        "                  batch_size=256,\n",
        "                  shuffle=True,\n",
        "                  validation_data=(testing_data, testing_data))\n",
        "\n",
        "  output_encoded = encoder.predict(testing_data)\n",
        "\n",
        "  return output_encoded\n",
        "\n",
        "\n",
        "# apply PCA on the data \n",
        "def PCA(variance_target, training_data, validation_data, testing_data):\n",
        "\n",
        "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
        "    \n",
        "    sum_square_singular = np.sum(sigma**2)\n",
        "    \n",
        "    ratios = sigma**2/sum_square_singular\n",
        "    n_components = 0\n",
        "    explained_variance = 0\n",
        "    \n",
        "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
        "    for i in range(len(ratios)):\n",
        "        if explained_variance >= variance_target:\n",
        "            break\n",
        "        else: \n",
        "            n_components += 1\n",
        "            explained_variance += ratios[i]\n",
        "    \n",
        "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
        "\n",
        "\n",
        "def load_in_dataset_and_preprocess(explained_variance):\n",
        "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "    \n",
        "    # reshape the data \n",
        "    training_data = training_data.reshape(50000, 3072)\n",
        "    testing_data = testing_data.reshape(10000, 3072)\n",
        "\n",
        "    # preprocess data\n",
        "    validation_data = training_data[49000:, :]\n",
        "    validation_labels = np.squeeze(training_labels[49000:, :])\n",
        "    training_data = training_data[:49000, :]\n",
        "    training_labels = np.squeeze(training_labels[:49000, :])\n",
        "    \n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "    validation_data = validation_data.astype('float32')\n",
        "\n",
        "\n",
        "    # Centre data\n",
        "    training_data, testing_data, validation_data = centre_data(training_data, validation_data, testing_data)\n",
        "\n",
        "    #apply autoencoder\n",
        "    #training_data= autoencoder(training_data, testing_data)\n",
        "\n",
        "    #print(training_data.shape)\n",
        "\n",
        "    # Apply PCA\n",
        "    training_data, testing_data, validation_data = PCA(explained_variance, training_data, validation_data, testing_data)\n",
        "\n",
        "     #one hot encoding\n",
        "    #training_labels = to_categorical(training_labels, 20)\n",
        "    #testing_labels = to_categorical(testing_labels, 20)\n",
        "    \n",
        "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n",
        "\n",
        "training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
        "\n",
        "\n",
        "# Normalization of pixel values (to [0-1] range)\n",
        "training_data = training_data / 255\n",
        "testing_data = testing_data / 255\n",
        "\n",
        "# Normalization of pixel values (to [0-1] range)\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(256, activation='relu', input_dim=training_data.shape[1])) #training_data\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256,activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='softmax')) # will slow the classifier down significantly as you increase the dimensionality\n",
        "\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# can also use loss function categorical_crossentropy\n",
        "# or optimiser SGD\n",
        "# try with different optimisers and loss functions\n",
        "\n",
        "adam = Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(training_data, training_labels, epochs=50, batch_size=32, verbose=2, validation_split=0.2)\n",
        "score = model.evaluate(testing_data, testing_labels, batch_size=128, verbose=0)\n",
        "\n",
        "print(model.metrics_names)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGi0DKkJ0rtA"
      },
      "source": [
        "**ZCA- Image Whitening**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxVoi480xZc"
      },
      "source": [
        "\n",
        "def zca_whitening_matrix(matrix):\n",
        " \n",
        "    sigma = np.cov(matrix, row=True)\n",
        "    U,S,V = np.linalg.svd(sigma)\n",
        "    e = 1e-5\n",
        "    result = np.dot(U, np.dot(np.diag(1.0/np.sqrt(S + e)), U.T)) \n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m60acqkL1Tdo"
      },
      "source": [
        "**Feature Extraction with Autoencoders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvIdlzVG1avI"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import cifar100\n",
        "import numpy as np\n",
        "\n",
        "dimension = 32 \n",
        "input_image = keras.Input(shape=(3072,))\n",
        "\n",
        "encoded = layers.Dense(dimension, activation='relu')(input_image)\n",
        "decoded = layers.Dense(3072, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = keras.Model(input_image, decoded)\n",
        "encoder = keras.Model(input_image, encoded)\n",
        "input_encode = keras.Input(shape=(dimension,))\n",
        "layers = autoencoder.layers[-1]\n",
        "\n",
        "decoder = keras.Model(input_encode, layers(input_encode))\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "(training_data, _), (testing_data, _) = (cifar100.load_data(\"coarse\"))\n",
        "\n",
        "training_data = training_data.astype('float32') / 255.\n",
        "testing_data = testing_data.astype('float32') / 255.\n",
        "training_data = training_data.reshape((len(training_data), np.prod(training_data.shape[1:])))\n",
        "testing_data = testing_data.reshape((len(testing_data), np.prod(testing_data.shape[1:])))\n",
        "\n",
        "autoencoder.fit(training_data, training_data,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(testing_data, testing_data))\n",
        "\n",
        "output_encoded = encoder.predict(testing_data)\n",
        "print(output_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}