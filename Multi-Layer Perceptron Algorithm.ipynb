{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 2.4015, Accuracy: 2657/10000 (27%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Constants\n",
    "n_labels = 20\n",
    "image_dimensions = 32*32*3\n",
    "\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_units, n_hidden_layers, drop_rate=0.5):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        \n",
    "        # set up perceptron layers and add dropout, outputs linear transformation y = Wx + b\n",
    "        self.linear_function_1 = torch.nn.Linear(image_dimensions, n_hidden_units)\n",
    "        \n",
    "        # randomly zeroes some of the elements of the input tensor with probability p using samples from a \n",
    "        # Bernoulli distribution\n",
    "        self.linear_function_1_drop = torch.nn.Dropout(drop_rate)\n",
    "        \n",
    "        # if number of hidden layers is 2\n",
    "        if n_hidden_layers == 2:\n",
    "            \n",
    "            #TODO: don't know if I need these \n",
    "            self.linear_function_2 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_2_drop = torch.nn.Dropout(drop_rate)\n",
    "            \n",
    "        if n_hidden_layers == 3:\n",
    "            \n",
    "            self.linear_function_3 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_3_drop = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "        self.output = torch.nn.Linear(n_hidden_units, n_labels)\n",
    "\n",
    "    # feed forward the data \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        input_data = input_data.view(-1, image_dimensions)\n",
    "        \n",
    "        # input x is passed to fully connected layer, then step function elu is applied, makes it non linear\n",
    "        input_data = torch.nn.functional.elu(self.linear_function_1(input_data))\n",
    "        \n",
    "        input_data = self.linear_function_1_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 2:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_2(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_2_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 3:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_3(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_3_drop(input_data)\n",
    "        \n",
    "        # It is applied to all slices along dim, and will re-scale them so that \n",
    "        # the elements lie in the range [0, 1] and sum to 1\n",
    "        return torch.nn.functional.log_softmax(self.output(input_data), -1)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, log_interval=100):\n",
    "    \n",
    "    # switch the module mode to .train() so that new weights can be learned after every epoch\n",
    "    model.train()\n",
    "    \n",
    "    num_correct_predictions = 0\n",
    "    \n",
    "    # total number of data points \n",
    "    num_data_points = len(train_loader.dataset)\n",
    "    \n",
    "    for batch, (training_data, training_labels) in enumerate(train_loader):\n",
    "        \n",
    "        # sets the gradients to zero before we start back propogation \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pass training data into model \n",
    "        prediction = model(training_data)\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        best_prediction = prediction.data.max(1)[1] \n",
    "        \n",
    "        # determine number of correct predictions \n",
    "        num_correct_predictions += (best_prediction.eq(training_labels.data)).sum()\n",
    "        \n",
    "        accuracy = num_correct_predictions / num_data_points * 100.00\n",
    "        \n",
    "        # calculate negative log likelihood loss\n",
    "        loss = torch.nn.functional.nll_loss(prediction, training_labels)\n",
    "        \n",
    "        # automatically performs the back propogation \n",
    "        loss.backward()\n",
    "        \n",
    "        # updates the weights accordingly\n",
    "        optimizer.step()\n",
    "\n",
    "def validate_model(loss_vector, accuracy_vector, model, validation_loader):\n",
    "    \n",
    "    # the common practice for evaluating/validation is using torch.no_grad() \n",
    "    # in pair with model.eval() to turn off gradients computation\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_train, y_train in validation_loader:\n",
    "            \n",
    "            #x_train, y_train = Variable(x_train), Variable(y_train)\n",
    "            \n",
    "            output = model(x_train)\n",
    "            \n",
    "            # The negative log likelihood loss - (input, target) - \n",
    "            loss += torch.nn.functional.nll_loss(output, y_train).data\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            best_prediction = output.data.max(1)[1] \n",
    "            \n",
    "            # Compares two tensors element-wise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible\n",
    "            # .sum() Returns the sum of all elements in the input tensor.\n",
    "            correct_predictions += best_prediction.eq(y_train.data).sum()\n",
    "\n",
    "    loss /= len(validation_loader)\n",
    "    \n",
    "    accuracy = 100. * correct_predictions / len(validation_loader.dataset)\n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct_predictions, len(validation_loader.dataset), accuracy))\n",
    "    \n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as dataset:\n",
    "        cifar_100_dataset = pickle.load(dataset, encoding='bytes')\n",
    "    return cifar_100_dataset\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_training_data, training_coarse_labels, transform):\n",
    "        \n",
    "        self.train_labels = training_coarse_labels\n",
    "        \n",
    "        self.train_data = preprocessed_training_data\n",
    "        self.transform = transform\n",
    "        self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
    "        self.train_data = self.train_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of training samples\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.train_data[index]\n",
    "        target = self.train_labels[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        return img, target\n",
    "    \n",
    "\n",
    "class Test_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_testing_data, testing_coarse_labels, transform):\n",
    "        self.test_labels = testing_coarse_labels\n",
    "        self.test_data = preprocessed_testing_data\n",
    "        self.transform = transform\n",
    "        self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "        self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of testing samples \n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.test_data[index]\n",
    "    \n",
    "        target = self.test_labels[index]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def load_data_and_create_dataloaders():\n",
    "    \n",
    "    # transformations to be applied to data \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # load in data \n",
    "    meta = unpickle('cifar-100-python/meta')\n",
    "    train = unpickle('cifar-100-python/train')\n",
    "    test = unpickle('cifar-100-python/test')\n",
    "\n",
    "    # extract training labels \n",
    "    training_coarse_labels = train[b'coarse_labels']\n",
    "    \n",
    "    # extract training data\n",
    "    training_data = train[b'data']\n",
    "    \n",
    "    # extract test data\n",
    "    testing_data = test[b'data']\n",
    "    \n",
    "    # extract test labels\n",
    "    testing_coarse_labels = test[b'coarse_labels']\n",
    "    \n",
    "    # dataset for training model    \n",
    "    training_set = Train_Dataset(training_data, training_coarse_labels, transform=transform)\n",
    "    \n",
    "    # data loader for training set \n",
    "    train_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # dataset for testing model\n",
    "    testing_set = Test_Dataset(testing_data, testing_coarse_labels, transform=transform)\n",
    "    \n",
    "    # data loader for testing set \n",
    "    validation_loader = torch.utils.data.DataLoader(testing_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, validation_loader\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    train_loader, validation_loader = load_data_and_create_dataloaders()\n",
    "    \n",
    "    num_hidden_units = 200\n",
    "    num_hidden_layers = 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Perceptron(num_hidden_units, num_hidden_layers)\n",
    "\n",
    "    # stochastic gradient descent \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.75, weight_decay=.0005)\n",
    "\n",
    "    losses = [] \n",
    "    accuracies = []\n",
    "    \n",
    "    for iteration in range(1, 20):\n",
    "        \n",
    "        train_model(model, train_loader, optimizer)\n",
    "        \n",
    "        validate_model(losses, accuracies, model, validation_loader)\n",
    "        \n",
    "    total_time = (time.time() - start_time)/60\n",
    "    \n",
    "    print(\"Total time\", total_time)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
