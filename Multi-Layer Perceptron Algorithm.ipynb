{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3072)\n",
      "(10000, 3072)\n",
      "\n",
      "Validation set: Average loss: 78.4150, Accuracy: 500/10000 (5%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fb18e7b0f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3fb18e7b0f39>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fb18e7b0f39>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# automatically performs the back propogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# updates the weights accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "# Constants\n",
    "n_labels = 20\n",
    "# this should not be a constant, fix!\n",
    "\n",
    "#image_dimensions = 104\n",
    "\n",
    "\n",
    "# centre the data\n",
    "def centre_data(train, validation, test):\n",
    "    \n",
    "    # calculate the means for each attribute of the training data\n",
    "    column_means = np.mean(train, axis=0) \n",
    "    \n",
    "    # centre training data by subtracting training data attribute means\n",
    "    for i in range(len(train)):\n",
    "        train[i] = train[i] - column_means\n",
    "    \n",
    "    # centre testing data by subtracting training data attribute means\n",
    "    for x in range(len(test)):\n",
    "        test[x] = test[x] - column_means\n",
    "    \n",
    "    for x in range(len(validation)):\n",
    "        validation[x] = validation[x] - column_means\n",
    "        \n",
    "    return train, validation, test\n",
    "\n",
    "# apply PCA on the data \n",
    "def PCA(variance_target, training_data, validation_data, testing_data):\n",
    "\n",
    "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
    "    \n",
    "    sum_square_singular = np.sum(sigma**2)\n",
    "    \n",
    "    ratios = sigma**2/sum_square_singular\n",
    "    \n",
    "                \n",
    "    n_components = 0\n",
    "    explained_variance = 0\n",
    "    \n",
    "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
    "    for i in range(len(ratios)):\n",
    "        if explained_variance >= variance_target:\n",
    "            break\n",
    "        else: \n",
    "            n_components += 1\n",
    "            explained_variance += ratios[i]\n",
    "    \n",
    "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
    "\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_units, n_hidden_layers, image_dimensions, drop_rate=0.5):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.image_dimensions = image_dimensions\n",
    "    \n",
    "        \n",
    "        # set up perceptron layers and add dropout, outputs linear transformation y = Wx + b\n",
    "        self.linear_function_1 = torch.nn.Linear(self.image_dimensions, n_hidden_units)\n",
    "        \n",
    "        # randomly zeroes some of the elements of the input tensor with probability p using samples from a \n",
    "        # Bernoulli distribution\n",
    "        self.linear_function_1_drop = torch.nn.Dropout(drop_rate)\n",
    "        \n",
    "        # if number of hidden layers is 2\n",
    "        if n_hidden_layers == 2:\n",
    "            \n",
    "            #TODO: don't know if I need these \n",
    "            self.linear_function_2 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_2_drop = torch.nn.Dropout(drop_rate)\n",
    "            \n",
    "        if n_hidden_layers == 3:\n",
    "            \n",
    "            self.linear_function_3 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_3_drop = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "        self.output = torch.nn.Linear(n_hidden_units, n_labels)\n",
    "\n",
    "    # feed forward the data \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "\n",
    "        input_data = input_data.view(-1, self.image_dimensions)\n",
    "        \n",
    "        # input x is passed to fully connected layer, then step function elu is applied, makes it non linear\n",
    "        input_data = torch.nn.functional.elu(self.linear_function_1(input_data))\n",
    "        \n",
    "        input_data = self.linear_function_1_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 2:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_2(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_2_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 3:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_3(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_3_drop(input_data)\n",
    "        \n",
    "        # It is applied to all slices along dim, and will re-scale them so that \n",
    "        # the elements lie in the range [0, 1] and sum to 1\n",
    "        return torch.nn.functional.log_softmax(self.output(input_data), -1)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, log_interval=100):\n",
    "    \n",
    "    # switch the module mode to .train() so that new weights can be learned after every epoch\n",
    "    model.train()\n",
    "    \n",
    "    num_correct_predictions = 0\n",
    "    \n",
    "    # total number of data points \n",
    "    num_data_points = len(train_loader.dataset)\n",
    "    \n",
    "    for batch, (training_data, training_labels) in enumerate(train_loader):\n",
    "     \n",
    "        # sets the gradients to zero before we start back propogation \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pass training data into model \n",
    "        prediction = model(training_data.float())\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        best_prediction = prediction.data.max(1)[1] \n",
    "        \n",
    "        # determine number of correct predictions \n",
    "        num_correct_predictions += (best_prediction.eq(training_labels.data)).sum()\n",
    "        \n",
    "        accuracy = num_correct_predictions / num_data_points * 100.00\n",
    "      \n",
    "        # calculate negative log likelihood loss\n",
    "        loss = torch.nn.functional.nll_loss(prediction, training_labels)\n",
    "        \n",
    "        # automatically performs the back propogation \n",
    "        loss.backward()\n",
    "        \n",
    "        # updates the weights accordingly\n",
    "        optimizer.step()\n",
    "\n",
    "def validate_model(loss_vector, accuracy_vector, model, validation_loader):\n",
    "    \n",
    "    # the common practice for evaluating/validation is using torch.no_grad() \n",
    "    # in pair with model.eval() to turn off gradients computation\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_train, y_train in validation_loader:\n",
    "            \n",
    "            #x_train, y_train = Variable(x_train), Variable(y_train)\n",
    "            \n",
    "            output = model(x_train.float())\n",
    "            \n",
    "            # The negative log likelihood loss - (input, target) - \n",
    "            loss += torch.nn.functional.nll_loss(output, y_train).data\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            best_prediction = output.data.max(1)[1] \n",
    "            \n",
    "            # Compares two tensors element-wise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible\n",
    "            # .sum() Returns the sum of all elements in the input tensor.\n",
    "            correct_predictions += best_prediction.eq(y_train.data).sum()\n",
    "\n",
    "    loss /= len(validation_loader)\n",
    "    \n",
    "    accuracy = 100. * correct_predictions / len(validation_loader.dataset)\n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct_predictions, len(validation_loader.dataset), accuracy))\n",
    "    \n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as dataset:\n",
    "        cifar_100_dataset = pickle.load(dataset, encoding='bytes')\n",
    "    return cifar_100_dataset\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_training_data, training_coarse_labels, transform):\n",
    "        \n",
    "        self.train_labels = training_coarse_labels\n",
    "        \n",
    "        self.train_data = preprocessed_training_data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of training samples\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.train_data[index]\n",
    "        target = self.train_labels[index]\n",
    "#         img = Image.fromarray(img)\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "    \n",
    "        return img, target\n",
    "    \n",
    "\n",
    "class Test_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_testing_data, testing_coarse_labels, transform):\n",
    "        self.test_labels = testing_coarse_labels\n",
    "        self.test_data = preprocessed_testing_data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of testing samples \n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.test_data[index]\n",
    "    \n",
    "        target = self.test_labels[index]\n",
    "#         img = Image.fromarray(img)\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def load_data_and_create_dataloaders():\n",
    "    \n",
    "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "    \n",
    "#     testing_labels = np.squeeze(testing_labels)\n",
    "#     training_labels = np.squeeze(training_labels)\n",
    "    \n",
    "    # reshape the data \n",
    "    training_data = training_data.reshape(50000, 3072)\n",
    "    testing_data = testing_data.reshape(10000, 3072)\n",
    "\n",
    "   ## preprocess data\n",
    "    validation_data = training_data[49000:, :]\n",
    "    validation_labels = np.squeeze(training_labels[49000:, :])\n",
    "    training_data = training_data[:49000, :]\n",
    "    training_labels = np.squeeze(training_labels[:49000, :])\n",
    "    testing_labels = np.squeeze(testing_labels)\n",
    "    testing_data = testing_data\n",
    "\n",
    "#     Centre data\n",
    "    training_data, validation_data, testing_data = centre_data(training_data, validation_data, testing_data)\n",
    "\n",
    "#     Apply PCA\n",
    "    #training_data, testing_data, validation_data = PCA(0.99, training_data, validation_data, testing_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    #reshape the data back to the original shape\n",
    "    \n",
    "    print(training_data.shape)\n",
    "    image_dimensions = training_data.shape[1]\n",
    "    \n",
    "    print(testing_data.shape)\n",
    "    \n",
    "#     training_data.reshape(49000, 32, 32, 3) \n",
    "#     testing_data.reshape(10000, 32, 32, 3) \n",
    "\n",
    "#     training_data = training_data.reshape((49000, 3, 32, 32))\n",
    "#     training_data = training_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "#     testing_data = testing_data.reshape((10000, 3, 32, 32))\n",
    "#     testing_data = testing_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    # transformations to be applied to data \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    # dataset for training model    \n",
    "    training_set = Train_Dataset(training_data.astype(np.float), training_labels, transform=transform)\n",
    "    \n",
    "    \n",
    "\n",
    "    # data loader for training set \n",
    "    train_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # dataset for testing model\n",
    "    testing_set = Test_Dataset(testing_data.astype(np.float), testing_labels, transform=transform)\n",
    "    \n",
    "    # data loader for testing set \n",
    "    validation_loader = torch.utils.data.DataLoader(testing_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, validation_loader, image_dimensions\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    train_loader, validation_loader, image_dimensions = load_data_and_create_dataloaders()\n",
    "    \n",
    "    num_hidden_units = 200\n",
    "    num_hidden_layers = 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Perceptron(num_hidden_units, num_hidden_layers, image_dimensions)\n",
    "\n",
    "    # stochastic gradient descent \n",
    "    # could try different optimisers here \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.75, weight_decay=.0005)\n",
    "\n",
    "    losses = [] \n",
    "    accuracies = []\n",
    "    \n",
    "    for iteration in range(1, 20):\n",
    "        \n",
    "        train_model(model, train_loader, optimizer)\n",
    "        \n",
    "        validate_model(losses, accuracies, model, validation_loader)\n",
    "        \n",
    "    total_time = (time.time() - start_time)/60\n",
    "    \n",
    "    print(\"Total time\", total_time)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
