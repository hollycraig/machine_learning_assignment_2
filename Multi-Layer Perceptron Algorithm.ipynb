{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time 0.00010341405868530273\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "n_labels = 20\n",
    "image_dimensions = 32*32*3\n",
    "\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_units, n_hidden_layers, drop_rate=0.5):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        \n",
    "        # set up perceptron layers and add dropout, outputs linear transformation y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(image_dimensions, n_hidden_units)\n",
    "        \n",
    "        # randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distributio\n",
    "        self.fc1_drop = torch.nn.Dropout(drop_rate)\n",
    "        \n",
    "        if n_hidden_layers == 2:\n",
    "            \n",
    "            self.fc2 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            #probability of an element to be zeroed\n",
    "            self.fc2_drop = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "        self.out = torch.nn.Linear(n_hidden_units, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, image_dimensions)\n",
    "        \n",
    "        # input x is passed to fully connected layer, then step function elu is applied, makes it non linear\n",
    "        x = torch.nn.functional.elu(self.fc1(x))\n",
    "        \n",
    "        x = self.fc1_drop(x)\n",
    "        \n",
    "        if self.n_hidden_layers == 2:\n",
    "         \n",
    "            x = torch.nn.functional.elu(self.fc2(x))\n",
    "            \n",
    "            x = self.fc2_drop(x)\n",
    "            \n",
    "        return torch.nn.functional.log_softmax(self.out(x))\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, log_interval=100):\n",
    "    \n",
    "    # switch the module mode to .train() so that new weights can be learned after every epoch\n",
    "    model.train()\n",
    "    \n",
    "    num_correct_predictions = 0\n",
    "    \n",
    "    num_data_points = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # sets the gradients to zero before we start back propogation \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        best_prediction = y_pred.data.max(1)[1] \n",
    "        \n",
    "        num_correct_predictions += best_prediction.eq(y_train.data).sum()\n",
    "        \n",
    "        accuracy = num_correct_predictions / num_data_points * 100.00\n",
    "        \n",
    "        loss = torch.nn.functional.nll_loss(y_pred, y_train)\n",
    "        \n",
    "        # automatically performs the back propogation \n",
    "        loss.backward()\n",
    "        \n",
    "        # updates the weights accordingly\n",
    "        optimizer.step()\n",
    "\n",
    "def validate_model(loss_vector, accuracy_vector, model, validation_loader):\n",
    "    \n",
    "    # the common practice for evaluating/validation is using torch.no_grad() \n",
    "    # in pair with model.eval() to turn off gradients computation\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_train, y_train in validation_loader:\n",
    "            x_train, y_train = Variable(x_train), Variable(y_train)\n",
    "            \n",
    "            output = model(x_train)\n",
    "            \n",
    "            # The negative log likelihood loss - (input, target) - \n",
    "            loss += torch.nn.functional.nll_loss(output, y_train).data\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            best_prediction = output.data.max(1)[1] \n",
    "            \n",
    "            # Compares two tensors element-wise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible\n",
    "            # .sum() Returns the sum of all elements in the input tensor.\n",
    "            correct_predictions += best_prediction.eq(y_train.data).sum()\n",
    "\n",
    "    loss /= len(validation_loader)\n",
    "    \n",
    "    accuracy = 100. * correct_predictions / len(validation_loader.dataset)\n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct_predictions, len(validation_loader.dataset), accuracy))\n",
    "    \n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_training_data, training_fine_labels, transform):\n",
    "        self.train_labels = training_fine_labels\n",
    "        self.train_data = preprocessed_training_data\n",
    "        self.transform = transform\n",
    "        self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
    "        self.train_data = self.train_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of training samples\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.train_data[index]\n",
    "        target = self.train_labels[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        return img, target\n",
    "    \n",
    "\n",
    "class Test_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_testing_data, testing_fine_labels, transform):\n",
    "        self.test_labels = testing_fine_labels\n",
    "        self.test_data = preprocessed_testing_data\n",
    "        self.transform = transform\n",
    "        self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "        self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of testing samples \n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.test_data[index]\n",
    "    \n",
    "        target = self.test_labels[index]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def load_data_and_create_dataloaders():\n",
    "    \n",
    "    # transformations to be applied to data \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # load in data \n",
    "    meta = unpickle('cifar-100-python/meta')\n",
    "    train = unpickle('cifar-100-python/train')\n",
    "    test = unpickle('cifar-100-python/test')\n",
    "\n",
    "    # extract training labels \n",
    "    training_fine_labels = train[b'coarse_labels']\n",
    "    \n",
    "    # extract training data\n",
    "    training_data = train[b'data']\n",
    "    \n",
    "    # extract test data\n",
    "    testing_data = test[b'data']\n",
    "    \n",
    "    # extract test labels\n",
    "    testing_fine_labels = test[b'coarse_labels']\n",
    "    \n",
    "    # dataset for training model    \n",
    "    training_set = Train_Dataset(training_data, training_fine_labels, transform=transform)\n",
    "    \n",
    "    # data loader for training set \n",
    "    train_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # dataset for testing model\n",
    "    testing_set = Test_Dataset(testing_data, testing_fine_labels, transform=transform)\n",
    "    \n",
    "    # data loader for testing set \n",
    "    validation_loader = torch.utils.data.DataLoader(testing_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, validation_loader\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    train_loader, validation_loader = load_data_and_create_dataloaders()\n",
    "    \n",
    "    hidden_units = 200\n",
    "    layers = 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Perceptron(hidden_units, layers)\n",
    "\n",
    "    # stochastic gradient descent \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.75, weight_decay=.0005)\n",
    "\n",
    "    losses = [] \n",
    "    accuracies = []\n",
    "    \n",
    "    for iteration in range(1, 1):\n",
    "        \n",
    "        train_model(model, train_loader, optimizer)\n",
    "        \n",
    "        validate_model(losses, accuracies, model, validation_loader)\n",
    "        \n",
    "    total_time = (time.time() - start_time)/60\n",
    "    \n",
    "    print(\"Total time\", total_time)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
