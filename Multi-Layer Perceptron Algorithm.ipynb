{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Accuracy: 2208/10000 (22%)\n",
      "\n",
      "\n",
      "Validation set: Accuracy: 2454/10000 (25%)\n",
      "\n",
      "\n",
      "Validation set: Accuracy: 2494/10000 (25%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-56f1fc0d9522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-56f1fc0d9522>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# TODO: change validation stuff to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-56f1fc0d9522>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(classifier, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# updates the weights accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "# Constants\n",
    "n_labels = 20\n",
    "\n",
    "image_size = 32*32*3\n",
    "\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_units, n_hidden_layers, drop_rate=0.5):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        \n",
    "        # set up perceptron layers and add dropout, outputs linear transformation y = Wx + b\n",
    "        self.linear_function_1 = torch.nn.Linear(image_size, n_hidden_units)\n",
    "        \n",
    "        # randomly zeroes some of the elements of the input tensor with probability p using samples from a \n",
    "        # Bernoulli distribution\n",
    "        self.linear_function_1_drop = torch.nn.Dropout(drop_rate)\n",
    "        \n",
    "        # if number of hidden layers is 2\n",
    "        if n_hidden_layers == 2:\n",
    "            \n",
    "            #TODO: don't know if I need these \n",
    "            self.linear_function_2 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_2_drop = torch.nn.Dropout(drop_rate)\n",
    "            \n",
    "        if n_hidden_layers == 3:\n",
    "            \n",
    "            self.linear_function_3 = torch.nn.Linear(n_hidden_units, n_hidden_units)\n",
    "            \n",
    "            self.linear_function_3_drop = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "        self.output = torch.nn.Linear(n_hidden_units, n_labels)\n",
    "\n",
    "    # feed forward the data \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        #View tensor shares the same underlying data with its base tensor. \n",
    "        #Supporting View avoids explicit data copy, thus allows us to do fast and memory \n",
    "        # efficient reshaping, slicing and element-wise operations.\n",
    "        input_data = input_data.view(-1, image_size)\n",
    "        #input_data = torch.flatten(input_data)\n",
    "        \n",
    "        # input x is passed to fully connected layer, then step function elu is applied, makes it non linear\n",
    "        input_data = torch.nn.functional.elu(self.linear_function_1(input_data))\n",
    "        \n",
    "        input_data = self.linear_function_1_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 2:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_2(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_2_drop(input_data)\n",
    "        \n",
    "        if self.n_hidden_layers == 3:\n",
    "         \n",
    "            input_data = torch.nn.functional.elu(self.linear_function_3(input_data))\n",
    "            \n",
    "            input_data = self.linear_function_3_drop(input_data)\n",
    "        \n",
    "        # It is applied to all slices along dim, and will re-scale them so that \n",
    "        # the elements lie in the range [0, 1] and sum to 1\n",
    "        return torch.nn.functional.log_softmax(self.output(input_data), -1)\n",
    "\n",
    "\n",
    "def get_index_best_prediction(predictions):\n",
    "    index_best_prediction = predictions.max(1)[1] \n",
    "    return index_best_prediction\n",
    "\n",
    "\n",
    "\n",
    "def train_classifier(classifier, train_loader, optimizer, log_interval=100):\n",
    "    \n",
    "    # switch the module mode to .train() so that new weights can be learned after every epoch\n",
    "    classifier.train()\n",
    "    \n",
    "    num_correct_predictions = 0\n",
    "    \n",
    "    # total number of data points \n",
    "    num_data_points = len(train_loader.dataset)\n",
    "    \n",
    "    for batch, (training_data, training_labels) in enumerate(train_loader):\n",
    "     \n",
    "        # sets the gradients to zero before we start back propogation \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pass training data into model \n",
    "        prediction = classifier(training_data)\n",
    "        \n",
    "        predictions = prediction.data\n",
    "\n",
    "        # TODO: make this into a function - plus the one below it \n",
    "        # get the index of the max log-probability\n",
    "        best_prediction = predictions.max(1)[1] \n",
    "        \n",
    "        # determine number of correct predictions \n",
    "        num_correct_predictions += (best_prediction.eq(training_labels.data)).sum()\n",
    "        \n",
    "        accuracy = num_correct_predictions / num_data_points * 100.00\n",
    "        \n",
    "        # calculate negative log likelihood loss\n",
    "        loss = torch.nn.functional.nll_loss(prediction, training_labels)\n",
    "        \n",
    "        # automatically performs the back propogation \n",
    "        loss.backward()\n",
    "        \n",
    "        # updates the weights accordingly\n",
    "        optimizer.step()\n",
    "\n",
    "def calculate_predictions(classifier, validation_loader):\n",
    "    \n",
    "    # the common practice for evaluating/validation is using torch.no_grad() \n",
    "    # in pair with model.eval() to turn off gradients computation\n",
    "    classifier.eval()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for training_data, training_labels in validation_loader:\n",
    "            \n",
    "            output = classifier(training_data)\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            best_prediction = output.data.max(1)[1] \n",
    "            \n",
    "            # Compares two tensors element-wise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible\n",
    "            # .sum() Returns the sum of all elements in the input tensor.\n",
    "            correct_predictions += best_prediction.eq(training_labels.data).sum()\n",
    "\n",
    "    accuracy = 100. * correct_predictions / len(validation_loader.dataset)\n",
    "\n",
    "    print('\\nValidation set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct_predictions, len(validation_loader.dataset), accuracy))\n",
    "    \n",
    "\n",
    "# custom dataset object for training data \n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, training_data, training_labels, transform):\n",
    "        \n",
    "        self.training_labels = training_labels\n",
    "        \n",
    "        self.training_data = training_data\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image_array = self.training_data[i]\n",
    "        \n",
    "        target_label = self.training_labels[i]\n",
    "        \n",
    "        image = Image.fromarray(image_array)\n",
    "\n",
    "        transformed_image = self.transform(image)\n",
    "    \n",
    "        return transformed_image, target_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        # returns number of training samples \n",
    "        return len(self.training_data)\n",
    "    \n",
    "# custom dataset object for testing data\n",
    "class Test_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, testing_data, testing_labels, transform):\n",
    "        \n",
    "        self.testing_labels = testing_labels\n",
    "        \n",
    "        self.testing_data = testing_data\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image_array = self.testing_data[i]\n",
    "        \n",
    "        target_label = self.testing_labels[i]\n",
    "        \n",
    "        image = Image.fromarray(image_array)\n",
    "    \n",
    "        transformed_image = self.transform(image)\n",
    "\n",
    "        return transformed_image, target_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        # returns number of testing samples\n",
    "        return len(self.testing_data)\n",
    "    \n",
    "def load_data_and_create_dataloaders():\n",
    "    \n",
    "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "    \n",
    "    testing_labels = np.squeeze(testing_labels)\n",
    "    \n",
    "    training_labels = np.squeeze(training_labels)\n",
    "    \n",
    "    # transformations to be applied to data \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    # dataset for training model    \n",
    "    training_set = Train_Dataset(training_data, training_labels, transform=transform)\n",
    "\n",
    "    # data loader for training set \n",
    "    train_data_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # dataset for testing model\n",
    "    testing_set = Test_Dataset(testing_data, testing_labels, transform=transform)\n",
    "    \n",
    "    # data loader for testing set \n",
    "    test_data_loader = torch.utils.data.DataLoader(testing_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_data_loader, test_data_loader\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    train_data_loader, test_data_loader = load_data_and_create_dataloaders()\n",
    "    \n",
    "    num_hidden_units = 200\n",
    "    \n",
    "    num_hidden_layers = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    classifier = Perceptron(num_hidden_units, num_hidden_layers)\n",
    "\n",
    "    # stochastic gradient descent \n",
    "    # could try different optimisers here \n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=.75, weight_decay=.0005)\n",
    "    \n",
    "    num_iterations = 20\n",
    "    \n",
    "    for iteration in range(0, num_iterations):\n",
    "        \n",
    "        train_classifier(classifier, train_data_loader, optimizer)\n",
    "        \n",
    "        # TODO: change validation stuff to test \n",
    "        calculate_predictions(classifier, test_data_loader)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    total_time = (end_time - start_time)/60\n",
    "    \n",
    "    print(\"Total time\", total_time)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
