{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machine Class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyXHZWEo6Umx"
      },
      "source": [
        "**Support Vector Machine in a Class Form**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilJLBvE6g_Jr"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "class SVM:\n",
        "\n",
        "    training_data=[]\n",
        "    testing_data=[]\n",
        "\n",
        "    def __init__(self, training_data, testing_data):\n",
        "      self.training_data= training_data\n",
        "      self.testing_data= testing_data\n",
        "\n",
        "    def __len__(self, data):\n",
        "      return len(data)\n",
        "\n",
        "    def calculate_linear_output(self, data, weights):\n",
        "      return np.dot(data, weights)\n",
        "\n",
        "    # distance of point from separating hyperplane?\n",
        "    def calculate_distance(self, X, w):\n",
        "      return  y * (np.dot(X, w)) - 1\n",
        "\n",
        "    # calculate gradient, use L2 regularisation \n",
        "    def calculate_gradient(self, weights, regularisation_param, training_data, training_labels, num_classes):\n",
        "        \n",
        "        num_training_samples, num_training_features = training_data.shape\n",
        "        \n",
        "        gradient = np.zeros((num_training_features, num_classes))\n",
        "        \n",
        "        # w^Tx\n",
        "        linear_output = self.calculate_linear_output(training_data, weights)\n",
        "\n",
        "        #linear output with labels \n",
        "        linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
        "        delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
        "        \n",
        "        ones_and_zeros = np.zeros(delta.shape)\n",
        "        \n",
        "        # makes all the places where delta > 0, 1 else 0\n",
        "        # With lagrange multiplier considered, if the sample is on the support vector: ğ›¼ = 1\n",
        "        # else: ğ›¼ = 0\n",
        "        ones_and_zeros = np.where(delta > 0, 1, 0)\n",
        "        \n",
        "        # calculate the sum of each row \n",
        "        sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
        "        \n",
        "        ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
        "\n",
        "        gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
        "        \n",
        "        # controls the influence of each individual support vector on the objective function. \n",
        "        # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
        "        gradient = gradient + (2* regularisation_param * weights)\n",
        "        \n",
        "        return gradient \n",
        "\n",
        "    # train model using stochastic gradient descent \n",
        "    def train_model(self, training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
        "      \n",
        "      num_training_samples = len(training_data)\n",
        "      weights = weights\n",
        "\n",
        "      for i in range(iterations):\n",
        "      # create batch\n",
        "          batch = np.random.choice(num_training_samples, batch_size)\n",
        "          gradient = self.calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
        "          weights = weights - learning_rate * gradient\n",
        "\n",
        "      return weights\n",
        "\n",
        "    # calculate accuracy of model \n",
        "    def calculate_accuracy (self, data, labels, weights):\n",
        "        \n",
        "        accuracy = 0\n",
        "        prediction = np.zeros(len(data))\n",
        "\n",
        "      #w^Tx\n",
        "        linear_output= self.calculate_linear_output(data, weights)\n",
        "\n",
        "      # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
        "      # column index corresponding to the greatest index of each row\n",
        "        prediction = np.argmax(linear_output, axis=1)\n",
        "\n",
        "      # count the number of predictions that are correct \n",
        "        total_correct_predictions = (prediction == labels).sum()\n",
        "        num_data_points = len(data)\n",
        "        accuracy = (total_correct_predictions/num_data_points)*100\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "# centre the data\n",
        "def centre_data(train, validation, test):\n",
        "    \n",
        "    # calculate the means for each attribute of the training data\n",
        "    column_means = np.mean(train, axis=0) \n",
        "    \n",
        "    # centre training data by subtracting training data attribute means\n",
        "    for i in range(len(train)):\n",
        "        train[i] = train[i] - column_means\n",
        "    \n",
        "    # centre testing data by subtracting training data attribute means\n",
        "    for x in range(len(test)):\n",
        "        test[x] = test[x] - column_means\n",
        "        \n",
        "    for x in range(len(validation)):\n",
        "        validation[x] = validation[x] - column_means\n",
        "        \n",
        "    return train, test, validation\n",
        "\n",
        "# apply PCA on the data \n",
        "def PCA(variance_target, training_data, validation_data, testing_data):\n",
        "\n",
        "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
        "    \n",
        "    sum_square_singular = np.sum(sigma**2)\n",
        "    \n",
        "    ratios = sigma**2/sum_square_singular\n",
        "    n_components = 0\n",
        "    explained_variance = 0\n",
        "    \n",
        "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
        "    for i in range(len(ratios)):\n",
        "        if explained_variance >= variance_target:\n",
        "            break\n",
        "        else: \n",
        "            n_components += 1\n",
        "            explained_variance += ratios[i]\n",
        "    \n",
        "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
        "\n",
        "\n",
        "\n",
        "# helper function for concatenating labels onto their corresponding data points\n",
        "def concatenate_data(training_data, training_labels):\n",
        "    return np.column_stack((training_data, training_labels))\n",
        "\n",
        "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
        "def split_into_validation_training(training_matrix):\n",
        "    \n",
        "    import random\n",
        "    random.shuffle(training_matrix)\n",
        "\n",
        "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
        "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
        "    \n",
        "    return training_set, validation_set\n",
        "\n",
        "def load_in_dataset_and_preprocess(explained_variance):\n",
        "  \n",
        "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "    \n",
        "    # reshape the data \n",
        "    training_data = training_data.reshape(50000, 3072)\n",
        "    testing_data = testing_data.reshape(10000, 3072)\n",
        "\n",
        "    concatenated_training = concatenate_data(training_data, training_labels)\n",
        "\n",
        "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
        "\n",
        "    training_data = training_set[:, :-1]\n",
        "    training_labels = np.squeeze(training_set[:, -1])\n",
        "\n",
        "    validation_data = validation_set[:, :-1]\n",
        "    validation_labels = np.squeeze(validation_set[:, -1])\n",
        "\n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "    validation_data = validation_data.astype('float32')\n",
        "\n",
        "    # Centre data\n",
        "    training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
        "\n",
        "    number_training_samples = len(training_data)\n",
        "    number_validation_samples = len(validation_data)\n",
        "    number_testing_samples = len(testing_data)\n",
        "\n",
        "    # Reshape data from channel to rows\n",
        "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n",
        "\n",
        "\n",
        "training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
        "svm = SVM(training_data, testing_data)\n",
        "num_classes = np.max(training_labels) + 1\n",
        "weights = np.ones((len(training_data[0]), num_classes))\n",
        "weights= svm.train_model(training_data, training_labels, weights, 0.0000001, 5000, 15000, 200, 20)\n",
        "\n",
        "total_accuracy = svm.calculate_accuracy(validation_data, validation_labels, weights)\n",
        "print(total_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}