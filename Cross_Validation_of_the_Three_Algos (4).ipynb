{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross Validation of the Three Algos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTDgHAD1D3Kw"
      },
      "source": [
        "**Loading and Preprocessing - setting up for cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKatvt6gD80d"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "def flatten_data(x_train, y_train, x_test, y_test):\n",
        "    new_image_shape = 1\n",
        "    for dim in range(1, len(x_train.shape)):\n",
        "        new_image_shape *= x_train.shape[dim]\n",
        "        \n",
        "    flat_x_train = x_train.reshape((x_train.shape[0], new_image_shape))\n",
        "    flat_y_train = np.ravel(y_train)\n",
        "    \n",
        "    flat_x_test = x_test.reshape((x_test.shape[0], new_image_shape))\n",
        "    flat_y_test = np.ravel(y_test)\n",
        "    return flat_x_train, flat_y_train, flat_x_test, flat_y_test\n",
        "\n",
        "# centre the data\n",
        "def centre_data(train, test):\n",
        "    \n",
        "    # calculate the means for each attribute of the training data\n",
        "    column_means = np.mean(train, axis=0) \n",
        "    \n",
        "    # centre training data by subtracting training data attribute means\n",
        "    for i in range(len(train)):\n",
        "        train[i] = train[i] - column_means\n",
        "    \n",
        "    # centre testing data by subtracting training data attribute means\n",
        "    for x in range(len(test)):\n",
        "        test[x] = test[x] - column_means\n",
        "        \n",
        "    return train, test\n",
        "\n",
        "# apply PCA on the data \n",
        "def PCA(variance_target, training_data, testing_data):\n",
        "\n",
        "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
        "    \n",
        "    sum_square_singular = np.sum(sigma**2)\n",
        "    \n",
        "    ratios = sigma**2/sum_square_singular\n",
        "    n_components = 0\n",
        "    explained_variance = 0\n",
        "    \n",
        "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
        "    for i in range(len(ratios)):\n",
        "        if explained_variance >= variance_target:\n",
        "            break\n",
        "        else: \n",
        "            n_components += 1\n",
        "            explained_variance += ratios[i]\n",
        "    \n",
        "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components])\n",
        "\n",
        "\n",
        "\n",
        "def load_in_dataset_and_preprocess(explained_variance, training_data, testing_data, training_labels,testing_labels):\n",
        "\n",
        "    concatenated_training = concatenate_data(training_data, training_labels)\n",
        "\n",
        "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
        "\n",
        "    training_data = training_set[:, :-1]\n",
        "    training_labels = np.squeeze(training_set[:, -1])\n",
        "\n",
        "    validation_data = validation_set[:, :-1]\n",
        "    validation_labels = np.squeeze(validation_set[:, -1])\n",
        "\n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "    validation_data = validation_data.astype('float32')\n",
        "\n",
        "    # Centre data\n",
        "    #training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    #training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
        "\n",
        "    number_training_samples = len(training_data)\n",
        "    number_validation_samples = len(validation_data)\n",
        "    number_testing_samples = len(testing_data)\n",
        "\n",
        "    # Reshape data from channel to rows\n",
        "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "    return training_data, training_labels, testing_data, testing_labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72E1OmgD9XB"
      },
      "source": [
        "**Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUu4JuHSEAFQ",
        "outputId": "eeebf323-1d17-448d-94f7-2c47009d2889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "class SVM:\n",
        "\n",
        "    training_data=[]\n",
        "    testing_data=[]\n",
        "\n",
        "    def __init__(self, training_data, testing_data):\n",
        "      self.training_data= training_data\n",
        "      self.testing_data= testing_data\n",
        "\n",
        "    def __len__(self, data):\n",
        "      return len(data)\n",
        "\n",
        "    def calculate_linear_output(self, data, weights):\n",
        "      return np.dot(data, weights)\n",
        "\n",
        "    # distance of point from separating hyperplane?\n",
        "    def calculate_distance(self, X, w):\n",
        "      return  y * (np.dot(X, w)) - 1\n",
        "\n",
        "    # calculate gradient, use L2 regularisation \n",
        "    def calculate_gradient(self, weights, regularisation_param, training_data, training_labels, num_classes):\n",
        "        \n",
        "        num_training_samples, num_training_features = training_data.shape\n",
        "        \n",
        "        gradient = np.zeros((num_training_features, num_classes))\n",
        "        \n",
        "        # w^Tx\n",
        "        linear_output = self.calculate_linear_output(training_data, weights)\n",
        "\n",
        "        #linear output with labels \n",
        "        linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
        "        delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
        "        \n",
        "        ones_and_zeros = np.zeros(delta.shape)\n",
        "        \n",
        "        # makes all the places where delta > 0, 1 else 0\n",
        "        # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
        "        # else: ð›¼ = 0\n",
        "        ones_and_zeros = np.where(delta > 0, 1, 0)\n",
        "        \n",
        "        # calculate the sum of each row \n",
        "        sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
        "        \n",
        "        ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
        "\n",
        "        gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
        "        \n",
        "        # controls the influence of each individual support vector on the objective function. \n",
        "        # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
        "        gradient = gradient + (2* regularisation_param * weights)\n",
        "        \n",
        "        return gradient \n",
        "\n",
        "    # train model using stochastic gradient descent \n",
        "    def train_model(self, training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
        "      \n",
        "      num_training_samples = len(training_data)\n",
        "      weights = weights\n",
        "\n",
        "      for i in range(iterations):\n",
        "      # create batch\n",
        "          batch = np.random.choice(5000, batch_size) #change this to num_training_samples \n",
        "          gradient = self.calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
        "          weights = weights - learning_rate * gradient\n",
        "\n",
        "      return weights\n",
        "\n",
        "    # calculate accuracy of model \n",
        "    def calculate_accuracy (self, data, labels, weights):\n",
        "        \n",
        "        accuracy = 0\n",
        "        prediction = np.zeros(len(data))\n",
        "\n",
        "      #w^Tx\n",
        "        linear_output= self.calculate_linear_output(data, weights)\n",
        "\n",
        "      # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
        "      # column index corresponding to the greatest index of each row\n",
        "        prediction = np.argmax(linear_output, axis=1)\n",
        "\n",
        "      # count the number of predictions that are correct \n",
        "        total_correct_predictions = (prediction == labels).sum()\n",
        "        num_data_points = len(data)\n",
        "        accuracy = (total_correct_predictions/num_data_points)*100\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "# helper function for concatenating labels onto their corresponding data points\n",
        "def concatenate_data(training_data, training_labels):\n",
        "    return np.column_stack((training_data, training_labels))\n",
        "\n",
        "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
        "def split_into_validation_training(training_matrix):\n",
        "    \n",
        "    import random\n",
        "    random.shuffle(training_matrix)\n",
        "\n",
        "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
        "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
        "    \n",
        "    return training_set, validation_set\n",
        "\n",
        "#using 10 fold cross validation here to evaluate the performance of SVM\n",
        "def cross_validation():\n",
        "\n",
        "  (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "\n",
        "  # reshape the data \n",
        "  training_data = training_data.reshape(50000, 3072)\n",
        "  testing_data = testing_data.reshape(10000, 3072)\n",
        "  \n",
        "  concatenated_training = concatenate_data(training_data, training_labels)\n",
        "\n",
        "  training_set, validation_set = split_into_validation_training(concatenated_training)\n",
        "\n",
        "  training_data = training_set[:, :-1]\n",
        "  training_labels = np.squeeze(training_set[:, -1])\n",
        "\n",
        "  validation_data = validation_set[:, :-1]\n",
        "  validation_labels = np.squeeze(validation_set[:, -1])\n",
        "\n",
        "  training_data = training_data.astype('float32')\n",
        "  testing_data = testing_data.astype('float32')\n",
        "  validation_data = validation_data.astype('float32')\n",
        "\n",
        "  # Centre data\n",
        "  #training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
        "\n",
        "  # Apply PCA\n",
        "  #training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
        "\n",
        "  number_training_samples = len(training_data)\n",
        "  number_validation_samples = len(validation_data)\n",
        "  number_testing_samples = len(testing_data)\n",
        "\n",
        "  # Reshape data from channel to rows\n",
        "  training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "  validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "  testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "  #training_data, training_labels, testing_data, testing_labels = load_in_dataset_and_preprocess(0.9, training_data, training_labels, testing_data, testing_labels)\n",
        "  \n",
        "  cv = KFold(n_splits=10)\n",
        "\n",
        "  for train_index, test_index in cv.split(training_data):\n",
        "\n",
        "    training_set, training_set_labels = training_data[train_index], training_labels[train_index]\n",
        "    testing_set, testing_set_labels = training_data[test_index], training_labels[test_index]\n",
        "\n",
        "    svm = SVM(training_set, testing_set)\n",
        "    num_classes = np.max(training_set_labels) + 1\n",
        "    weights = np.ones((len(training_set[1]), num_classes))\n",
        "    weights= svm.train_model(training_set, training_set_labels, weights, 0.00000001, 1000, 20000, 200, 20)\n",
        "\n",
        "    total_accuracy = svm.calculate_accuracy(testing_set, testing_set_labels, weights)\n",
        "    print('accuracy: ', total_accuracy)\n",
        "\n",
        "cross_validation()\n",
        "\n",
        "  #run the classifiers here "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy:  34.91428571428571\n",
            "accuracy:  32.74285714285714\n",
            "accuracy:  28.142857142857142\n",
            "accuracy:  24.942857142857143\n",
            "accuracy:  24.257142857142856\n",
            "accuracy:  22.8\n",
            "accuracy:  20.82857142857143\n",
            "accuracy:  21.02857142857143\n",
            "accuracy:  22.142857142857142\n",
            "accuracy:  21.428571428571427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6hAGTTEAY5"
      },
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU525n67EC6W"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils.np_utils import to_categorical  \n",
        "from keras.datasets import cifar100\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import numpy as np\n",
        "\n",
        "def run_MLP_model(training_data, training_labels, testing_data, testing_labels, first_activation_function, second_activation_function, num_hidden_units, learning_rate, optimiser, decay_level, momentum, epochs, loss_function):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(num_hidden_units, activation=first_activation_function, input_dim=training_data.shape[1]))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_hidden_units, activation=first_activation_function))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(20, activation=second_activation_function))\n",
        "\n",
        "    if (optimiser == 'SGD'):\n",
        "        op = SGD(lr=learning_rate, decay=decay_level, momentum=momentum, nesterov=True)\n",
        "\n",
        "    else:\n",
        "        op = Adam(lr=learning_rate, decay=decay_level)\n",
        "\n",
        "    # can also use loss function categorical_crossentropy\n",
        "    # or optimiser SGD\n",
        "    # try with different optimisers and loss functions\n",
        "    model.compile(optimizer=op,\n",
        "                  loss=loss_function,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(training_data, training_labels, epochs=epochs, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "    score = model.evaluate(testing_data, testing_labels, batch_size=128, verbose=0)\n",
        "    return score[1]\n",
        "\n",
        "\n",
        "# helper function for concatenating labels onto their corresponding data points\n",
        "def concatenate_data(training_data, training_labels):\n",
        "    return np.column_stack((training_data, training_labels))\n",
        "\n",
        "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
        "def split_into_validation_training(training_matrix):\n",
        "    \n",
        "    import random\n",
        "    random.shuffle(training_matrix)\n",
        "\n",
        "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
        "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
        "    \n",
        "    return training_set, validation_set\n",
        "\n",
        "#using 10 fold cross validation here to evaluate the performance of SVM\n",
        "def cross_validation():\n",
        "\n",
        "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "\n",
        "    momentum = 0.9\n",
        "    decay=1e-06\n",
        "    learning_rate = 0.001\n",
        "    first_activation_function = 'relu'\n",
        "    second_activation_function = 'softmax'\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    optimiser = 'Adam'\n",
        "    epochs = 20\n",
        "    num_hidden_units = 256\n",
        "\n",
        "    # reshape the data \n",
        "    training_data = training_data.reshape(50000, 3072)\n",
        "    testing_data = testing_data.reshape(10000, 3072)\n",
        "\n",
        "    concatenated_training = concatenate_data(training_data, training_labels)\n",
        "\n",
        "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
        "\n",
        "    training_data = training_set[:, :-1]\n",
        "    training_labels = np.squeeze(training_set[:, -1])\n",
        "\n",
        "    validation_data = validation_set[:, :-1]\n",
        "    validation_labels = np.squeeze(validation_set[:, -1])\n",
        "\n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "    validation_data = validation_data.astype('float32')\n",
        "\n",
        "    # Centre data\n",
        "    training_data, testing_data = centre_data(training_data, testing_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    training_data, testing_data = PCA(0.9, training_data, testing_data)\n",
        "\n",
        "    number_training_samples = len(training_data)\n",
        "    number_validation_samples = len(validation_data)\n",
        "    number_testing_samples = len(testing_data)\n",
        "\n",
        "    # Reshape data from channel to rows\n",
        "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "    # Normalization of pixel values (to [0-1] range)\n",
        "    training_data = training_data / 255\n",
        "    testing_data = testing_data / 255\n",
        "    validation_data = validation_data / 255\n",
        "\n",
        "    cv = KFold(n_splits=10)\n",
        "\n",
        "    for train_index, test_index in cv.split(training_data):\n",
        "\n",
        "      training_set, training_set_labels = training_data[train_index], training_labels[train_index]\n",
        "      testing_set, testing_set_labels = training_data[test_index], training_labels[test_index]\n",
        "\n",
        "      accuracy= run_MLP_model(training_set, training_set_labels, testing_set, testing_set_labels, first_activation_function, second_activation_function, num_hidden_units, learning_rate, optimiser, decay, momentum, epochs, loss)\n",
        "      print('accuracy: ', accuracy)\n",
        "\n",
        "\n",
        "cross_validation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52gJ6_roEDSF"
      },
      "source": [
        "**Random Forests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGlmq6J8EFMs",
        "outputId": "cd634afd-4df0-4214-c073-ffa6ad724a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from skimage import exposure\n",
        "from skimage import feature\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def flatten_data(x_train, y_train, x_test, y_test):\n",
        "    new_image_shape = 1\n",
        "    for dim in range(1, len(x_train.shape)):\n",
        "        new_image_shape *= x_train.shape[dim]\n",
        "        \n",
        "    flat_x_train = x_train.reshape((x_train.shape[0], new_image_shape))\n",
        "    flat_y_train = np.ravel(y_train)\n",
        "    \n",
        "    flat_x_test = x_test.reshape((x_test.shape[0], new_image_shape))\n",
        "    flat_y_test = np.ravel(y_test)\n",
        "    return flat_x_train, flat_y_train, flat_x_test, flat_y_test\n",
        "\n",
        "def cross_validation():\n",
        "\n",
        "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "    cv = KFold(n_splits=10)\n",
        "\n",
        "    momentum = 0.9\n",
        "    decay=1e-6\n",
        "    learning_rate = 0.001\n",
        "    first_activation_function = 'relu'\n",
        "    second_activation_function = 'softmax'\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    optimiser = 'Adam'\n",
        "    epochs = 20\n",
        "    num_hidden_units = 256\n",
        "\n",
        "    training_data, training_labels, testing_data, testing_labels= flatten_data(training_data, training_labels, testing_data, testing_labels)\n",
        "\n",
        "    cv = KFold(n_splits=10)\n",
        "\n",
        "    for train_index, test_index in cv.split(training_data):\n",
        "\n",
        "      training_set, training_set_labels = training_data[train_index], training_labels[train_index]\n",
        "      testing_set, testing_set_labels = training_data[test_index], training_labels[test_index]\n",
        "      \n",
        "      model = RandomForestClassifier(\n",
        "      n_jobs=-1, \n",
        "      verbose=1,\n",
        "      n_estimators=400,\n",
        "      bootstrap=False, \n",
        "      max_features='sqrt', \n",
        "      criterion='gini')\n",
        "\n",
        "      model.fit(training_set, training_set_labels)\n",
        "      accuracy= model.score(testing_set, testing_set_labels)\n",
        "      print('accuracy: ',accuracy)\n",
        "\n",
        "\n",
        "cross_validation()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 22.1min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.8min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.6min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 22.1min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    2.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.5min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.3min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.9min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.3min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.1min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.3min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.1min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    2.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.4min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 21.2min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    2.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 20.8min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    1.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.3614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:    1.9s finished\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}