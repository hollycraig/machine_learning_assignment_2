{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4CD5VkiV-jX0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# centre the data\n",
    "def centre_data(train, validation, test):\n",
    "    \n",
    "    # calculate the means for each attribute of the training data\n",
    "    column_means = np.mean(train, axis=0) \n",
    "    \n",
    "    # centre training data by subtracting training data attribute means\n",
    "    for i in range(len(train)):\n",
    "        train[i] = train[i] - column_means\n",
    "    \n",
    "    # centre testing data by subtracting training data attribute means\n",
    "    for x in range(len(test)):\n",
    "        test[x] = test[x] - column_means\n",
    "        \n",
    "    for x in range(len(validation)):\n",
    "        validation[x] = validation[x] - column_means\n",
    "        \n",
    "    return train, test, validation\n",
    "\n",
    "# apply PCA on the data \n",
    "def PCA(variance_target, training_data, validation_data, testing_data):\n",
    "\n",
    "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
    "    \n",
    "    sum_square_singular = np.sum(sigma**2)\n",
    "    \n",
    "    ratios = sigma**2/sum_square_singular\n",
    "    n_components = 0\n",
    "    explained_variance = 0\n",
    "    \n",
    "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
    "    for i in range(len(ratios)):\n",
    "        if explained_variance >= variance_target:\n",
    "            break\n",
    "        else: \n",
    "            n_components += 1\n",
    "            explained_variance += ratios[i]\n",
    "    \n",
    "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
    "\n",
    "\n",
    "# calculate gradient, use L2 regularisation \n",
    "def calculate_gradient(weights, regularisation_param, training_data, training_labels, num_classes):\n",
    "    \n",
    "    num_training_samples, num_training_features = training_data.shape\n",
    "    \n",
    "    gradient = np.zeros(num_training_features, num_classes)\n",
    "    \n",
    "    # w^Tx\n",
    "    linear_output = np.dot(training_data, weights)\n",
    "\n",
    "    #linear output with labels \n",
    "    linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
    "  # distance of point from separating hyperplane?\n",
    "    # np.newaxis here makes it a column vector \n",
    "    # calculate distance?\n",
    "    \n",
    "    # distances = y * (np.dot(X, w)) - 1\n",
    "    delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
    "    \n",
    "    ones_and_zeros = np.zeros(delta.shape)\n",
    "    \n",
    "    # makes all the places where delta > 0, 1 else 0\n",
    "    # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
    "    # else: ð›¼ = 0\n",
    "    ones_and_zeros = np.where(delta > 0, 1, 0)\n",
    "    \n",
    "    # calculate the sum of each row \n",
    "    sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
    "    \n",
    "    ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
    "\n",
    "    gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
    "    \n",
    "    # controls the influence of each individual support vector on the objective function. \n",
    "    # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
    "    gradient = gradient + (2* regularisation_param * weights)\n",
    "    \n",
    "    return gradient \n",
    "\n",
    "# train model using stochastic gradient descent \n",
    "def train_model(training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size, num_classes):\n",
    "  \n",
    "  # number of examples in each batch\n",
    "  #batch_size = 200\n",
    "    num_training_samples = len(training_data)\n",
    "    weights = weights\n",
    "    for i in range(iterations):\n",
    "    # create batch\n",
    "        batch = np.random.choice(num_training_samples, batch_size)\n",
    "        gradient = calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch], num_classes)\n",
    "        weights = weights - learning_rate * gradient\n",
    "    return weights\n",
    "\n",
    "# calculate accuracy of model \n",
    "def calculate_accuracy (data, labels, weights):\n",
    "    accuracy = 0\n",
    "\n",
    "    prediction = np.zeros(len(data))\n",
    "\n",
    "  #w^Tx\n",
    "    linear_output = np.dot(data, weights)\n",
    "\n",
    "  # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
    "  # column index corresponding to the greatest index of each row\n",
    "    prediction = np.argmax(linear_output, axis=1)\n",
    "\n",
    "  # count the number of predictions that are correct \n",
    "    total_correct_predictions = (prediction == labels).sum()\n",
    "    num_data_points = len(data)\n",
    "\n",
    "    accuracy = (total_correct_predictions/num_data_points)*100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "def increase_image_contrast(image):\n",
    "    xp = [0, 64, 128, 192, 255]\n",
    "    fp = [0, 16, 128, 240, 255]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def increase_all_contrast(train, test):\n",
    "    new_train = np.copy(train)\n",
    "    new_test = np.copy(test)\n",
    "    for i in range(len(train)):\n",
    "        new_train[i] = increase_image_contrast(train[i])\n",
    "    for i in range(len(test)):\n",
    "        new_test[i] = increase_image_contrast(test[i])\n",
    "    return new_train, new_test\n",
    "\n",
    "# helper function for concatenating labels onto their corresponding data points\n",
    "def concatenate_data(training_data, training_labels):\n",
    "    return np.column_stack((training_data, training_labels))\n",
    "\n",
    "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
    "def split_into_validation_training(training_matrix):\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(training_matrix)\n",
    "\n",
    "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
    "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
    "    \n",
    "    return training_set, validation_set\n",
    "\n",
    "def load_in_dataset_and_preprocess(explained_variance):\n",
    "  \n",
    "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "    \n",
    "    # reshape the data \n",
    "    training_data = training_data.reshape(50000, 3072)\n",
    "    testing_data = testing_data.reshape(10000, 3072)\n",
    "\n",
    "    concatenated_training = concatenate_data(training_data, training_labels)\n",
    "\n",
    "    training_set, validation_set = split_into_validation_training(concatenated_training)\n",
    "\n",
    "    training_data = training_set[:, :-1]\n",
    "    training_labels = np.squeeze(training_set[:, -1])\n",
    "\n",
    "    validation_data = validation_set[:, :-1]\n",
    "    validation_labels = np.squeeze(validation_set[:, -1])\n",
    "\n",
    "    training_data = training_data.astype('float32')\n",
    "    testing_data = testing_data.astype('float32')\n",
    "    validation_data = validation_data.astype('float32')\n",
    "\n",
    "    # Centre data\n",
    "    training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
    "\n",
    "    # Apply PCA\n",
    "    training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
    "\n",
    "    number_training_samples = len(training_data)\n",
    "    number_validation_samples = len(validation_data)\n",
    "    number_testing_samples = len(testing_data)\n",
    "\n",
    "    # Reshape data from channel to rows\n",
    "    training_data = np.reshape(training_data, (number_training_samples, -1))\n",
    "    validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
    "    testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
    "\n",
    "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRsMcwbh_HEe"
   },
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZRjlzchTOzRO"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8ZcTXNp_N-Y"
   },
   "source": [
    "**Testing weight parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ETX6HfcGiW5T"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def nano_to_seconds(nanoseconds):\n",
    "    \"\"\"Converts nanoseconds to seconds rounded to the nearest 5 decimal places.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nanoseconds : int\n",
    "        The nanoseconds to convert\n",
    "    \"\"\"\n",
    "\n",
    "    return np.round((nanoseconds / 1e+9), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28mpQxaC_YgJ"
   },
   "source": [
    "**Testing learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NXxtdOoZ_fX6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.231428571428573\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "def test_svm_learning_rate():\n",
    "    \n",
    "    training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
    "    \n",
    "    num_classes = np.max(training_labels) + 1\n",
    "\n",
    "    learning_rates = [0.00000001]\n",
    "    \n",
    "    weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "    with open('svm_learning_rates.csv', mode='w', newline='') as csv_file:\n",
    "    \n",
    "        result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
    "        reg_param= 50000\n",
    "        iterations= 15000\n",
    "        batch_size=200\n",
    "\n",
    "        for n_learning_rate in learning_rates:\n",
    "                num_tests = 0\n",
    "                total_accuracy = 0\n",
    "                total_runtime = 0\n",
    "\n",
    "                for i in range(2):\n",
    "                    start_time = time.time()\n",
    "                    res_weights = train_model(training_data, training_labels, weights, n_learning_rate, reg_param, iterations,batch_size, num_classes)\n",
    "                    total_accuracy += calculate_accuracy(training_data, training_labels, res_weights)\n",
    "                    print(total_accuracy)\n",
    "                    training_time = time.time() - startTime\n",
    "                    total_runtime += nano_to_seconds(training_time)\n",
    "                    num_tests += 1\n",
    "\n",
    "                avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
    "                avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
    "                result_writer.writerow([weights, n_learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
    "\n",
    "test_svm_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYG50PBA_iZp"
   },
   "source": [
    "**Testing regularisation parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqQJOjDB_rUv"
   },
   "source": [
    "**Testing iterations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DyixbQI_4lL"
   },
   "source": [
    "**Testing batch size**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Support Vector Machine With Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
