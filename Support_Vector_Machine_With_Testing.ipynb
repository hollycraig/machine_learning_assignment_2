{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machine With Testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CD5VkiV-jX0"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# centre the data\n",
        "def centre_data(train, validation, test):\n",
        "    \n",
        "    # calculate the means for each attribute of the training data\n",
        "    column_means = np.mean(train, axis=0) \n",
        "    \n",
        "    # centre training data by subtracting training data attribute means\n",
        "    for i in range(len(train)):\n",
        "        train[i] = train[i] - column_means\n",
        "    \n",
        "    # centre testing data by subtracting training data attribute means\n",
        "    for x in range(len(test)):\n",
        "        test[x] = test[x] - column_means\n",
        "        \n",
        "    for x in range(len(validation)):\n",
        "        validation[x] = validation[x] - column_means\n",
        "        \n",
        "    return train, test, validation\n",
        "\n",
        "# apply PCA on the data \n",
        "def PCA(variance_target, training_data, validation_data, testing_data):\n",
        "\n",
        "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
        "    \n",
        "    sum_square_singular = np.sum(sigma**2)\n",
        "    \n",
        "    ratios = sigma**2/sum_square_singular\n",
        "    n_components = 0\n",
        "    explained_variance = 0\n",
        "    \n",
        "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
        "    for i in range(len(ratios)):\n",
        "        if explained_variance >= variance_target:\n",
        "            break\n",
        "        else: \n",
        "            n_components += 1\n",
        "            explained_variance += ratios[i]\n",
        "    \n",
        "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
        "\n",
        "\n",
        "# calculate gradient, use L2 regularisation \n",
        "def calculate_gradient(weights, regularisation_param, training_data, training_labels):\n",
        "    \n",
        "    num_training_samples, num_training_features = training_data.shape\n",
        "    \n",
        "    gradient = np.zeros(num_training_features, num_classes)\n",
        "    \n",
        "    # w^Tx\n",
        "    linear_output = np.dot(training_data, weights)\n",
        "\n",
        "    #linear output with labels \n",
        "    linear_output_y_i = linear_output[np.arange(num_training_samples),training_labels]\n",
        "  # distance of point from separating hyperplane?\n",
        "    # np.newaxis here makes it a column vector \n",
        "    # calculate distance?\n",
        "    \n",
        "    # distances = y * (np.dot(X, w)) - 1\n",
        "    delta = linear_output - linear_output_y_i[:,np.newaxis] + 1\n",
        "    \n",
        "    ones_and_zeros = np.zeros(delta.shape)\n",
        "    \n",
        "    # makes all the places where delta > 0, 1 else 0\n",
        "    # With lagrange multiplier considered, if the sample is on the support vector: 𝛼 = 1\n",
        "    # else: 𝛼 = 0\n",
        "    ones_and_zeros = np.where(delta > 0, 1, 0)\n",
        "    \n",
        "    # calculate the sum of each row \n",
        "    sum_of_each_row = np.sum(ones_and_zeros, axis=1)\n",
        "    \n",
        "    ones_and_zeros[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
        "\n",
        "    gradient = (1/num_training_samples) * np.dot((training_data.T), ones_and_zeros)\n",
        "    \n",
        "    # controls the influence of each individual support vector on the objective function. \n",
        "    # Greater C decreases the effect of |w|²/2, and results in the narrower margin\n",
        "    gradient = gradient + (2* regularisation_param * weights)\n",
        "    \n",
        "    return gradient \n",
        "\n",
        "# train model using stochastic gradient descent \n",
        "def train_model(training_data, training_labels, weights, learning_rate, regularisation_param, iterations, batch_size):\n",
        "  \n",
        "  # number of examples in each batch\n",
        "  #batch_size = 200\n",
        "  num_training_samples = len(training_data)\n",
        "  weights = weights\n",
        "  for i in range(iterations):\n",
        "    # create batch\n",
        "    batch = np.random.choice(num_training_samples, batch_size)\n",
        "    gradient = calculate_gradient(weights, regularisation_param, training_data[batch], training_labels[batch])\n",
        "    weights = weights - learning_rate * gradient\n",
        "  return weights\n",
        "\n",
        "# calculate accuracy of model \n",
        "def calculate_accuracy (data, labels, weights):\n",
        "  accuracy = 0\n",
        "\n",
        "  prediction = np.zeros(len(data))\n",
        "\n",
        "  #w^Tx\n",
        "  linear_output = np.dot(data, weights)\n",
        "\n",
        "  # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
        "  # column index corresponding to the greatest index of each row\n",
        "  prediction = np.argmax(linear_output, axis=1)\n",
        "\n",
        "  # count the number of predictions that are correct \n",
        "  total_correct_predictions = (prediction == labels).sum()\n",
        "  num_data_points = len(data)\n",
        "\n",
        "  accuracy = (total_correct_predictions/num_data_points)*100\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "def increase_image_contrast(image):\n",
        "    xp = [0, 64, 128, 192, 255]\n",
        "    fp = [0, 16, 128, 240, 255]\n",
        "    x = np.arange(256)\n",
        "    table = np.interp(x, xp, fp).astype('uint8')\n",
        "    return cv2.LUT(image, table)\n",
        "\n",
        "def increase_all_contrast(train, test):\n",
        "    new_train = np.copy(train)\n",
        "    new_test = np.copy(test)\n",
        "    for i in range(len(train)):\n",
        "        new_train[i] = increase_image_contrast(train[i])\n",
        "    for i in range(len(test)):\n",
        "        new_test[i] = increase_image_contrast(test[i])\n",
        "    return new_train, new_test\n",
        "\n",
        "# helper function for concatenating labels onto their corresponding data points\n",
        "def concatenate_data(training_data, training_labels):\n",
        "    return np.column_stack((training_data, training_labels))\n",
        "\n",
        "# data set is randomised and then split in a 70:30 ratio for training:validation sets\n",
        "def split_into_validation_training(training_matrix):\n",
        "    \n",
        "    import random\n",
        "    random.shuffle(training_matrix)\n",
        "\n",
        "    training_set = training_matrix[:int(len(training_matrix)*0.7)]\n",
        "    validation_set = training_matrix[int(len(training_matrix)*0.7):]\n",
        "    \n",
        "    return training_set, validation_set\n",
        "\n",
        "def load_in_dataset_and_preprocess(explained_variance):\n",
        "  \n",
        "  (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "   # reshape the data \n",
        "  training_data = training_data.reshape(50000, 3072)\n",
        "  testing_data = testing_data.reshape(10000, 3072)\n",
        "\n",
        "  concatenated_training = concatenate_data(training_data, training_labels)\n",
        "\n",
        "  training_set, validation_set = split_into_validation_training(concatenated_training)\n",
        "\n",
        "  training_data = training_set[:, :-1]\n",
        "  training_labels = np.squeeze(training_set[:, -1])\n",
        "\n",
        "  validation_data = validation_set[:, :-1]\n",
        "  validation_labels = np.squeeze(validation_set[:, -1])\n",
        "\n",
        "  training_data = training_data.astype('float32')\n",
        "  testing_data = testing_data.astype('float32')\n",
        "  validation_data = validation_data.astype('float32')\n",
        "\n",
        "  #     # Centre data\n",
        "  training_data, testing_data, validation_data = centre_data(training_data, testing_data, validation_data)\n",
        "\n",
        "  #     # Apply PCA\n",
        "  training_data, testing_data, validation_data = PCA(explained_variance, training_data, testing_data, validation_data)\n",
        "\n",
        "  # Normalization of pixel values (to [0-1] range)\n",
        "  training_data = training_data / 255\n",
        "  testing_data = testing_data / 255\n",
        "  validation_data = validation_data / 255\n",
        "\n",
        "  return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n",
        "\n",
        "def load_in_dataset_and_preprocess1(explained_variance):\n",
        "    (training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
        "    \n",
        "    # reshape the data \n",
        "    training_data = training_data.reshape(50000, 3072)\n",
        "    testing_data = testing_data.reshape(10000, 3072)\n",
        "\n",
        "    # preprocess data\n",
        "    validation_data = training_data[49000:, :]\n",
        "    validation_labels = np.squeeze(training_labels[49000:, :])\n",
        "    training_data = training_data[:49000, :]\n",
        "    training_labels = np.squeeze(training_labels[:49000, :])\n",
        "    \n",
        "    training_data = training_data.astype('float32')\n",
        "    testing_data = testing_data.astype('float32')\n",
        "    validation_data = validation_data.astype('float32')\n",
        "\n",
        "    # Centre data\n",
        "    training_data, testing_data, validation_data = centre_data(training_data, validation_data, testing_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    training_data, testing_data, validation_data = PCA(explained_variance, training_data, validation_data, testing_data)\n",
        "    \n",
        "    return training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels\n",
        "\n",
        "#training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
        "\n",
        "training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess1(0.8)\n",
        "\n",
        "training_labels = np.squeeze(training_labels)\n",
        "testing_labels = np.squeeze(testing_labels)\n",
        "validation_labels = np.squeeze(validation_labels)\n",
        "\n",
        "number_training_samples = len(training_data)\n",
        "number_validation_samples = len(validation_data)\n",
        "number_testing_samples = len(testing_data)\n",
        "\n",
        "# Reshape data from channel to rows\n",
        "training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "num_classes = np.max(training_labels) + 1\n",
        "\n",
        "weights = np.ones((len(training_data[0]), num_classes))\n",
        "\n",
        "weights = train_model(training_data, training_labels, weights, 0.00000001, 50000, 15000,200)\n",
        "print ('Training time: {0}'.format(time.time() - startTime))\n",
        "print ('Training acc:   {0}%'.format(calculate_accuracy(training_data, training_labels, weights)))\n",
        "print ('Validating acc: {0}%'.format(calculate_accuracy(validation_data, validation_labels, weights)))\n",
        "print ('Testing acc:    {0}%'.format(calculate_accuracy(testing_data, testing_labels, weights)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRsMcwbh_HEe"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRjlzchTOzRO"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8ZcTXNp_N-Y"
      },
      "source": [
        "**Testing weight parameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETX6HfcGiW5T"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import csv\n",
        "\n",
        "def nano_to_seconds(nanoseconds):\n",
        "    \"\"\"Converts nanoseconds to seconds rounded to the nearest 5 decimal places.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    nanoseconds : int\n",
        "        The nanoseconds to convert\n",
        "    \"\"\"\n",
        "\n",
        "    return np.round((nanoseconds / 1e+9), 5)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Vl0eDE_U1w"
      },
      "source": [
        "\n",
        "def test_svm_weights():\n",
        "  \n",
        "  (raw_x_train, raw_y_train), (raw_x_test, raw_y_test) = import_data()\n",
        "  training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess1(0.8)\n",
        "    \n",
        "  training_labels = np.squeeze(training_labels)\n",
        "  testing_labels = np.squeeze(testing_labels)\n",
        "  validation_labels = np.squeeze(validation_labels)\n",
        "\n",
        "  number_training_samples = len(training_data)\n",
        "  number_validation_samples = len(validation_data)\n",
        "  number_testing_samples = len(testing_data)\n",
        "\n",
        "  # Reshape data from channel to rows\n",
        "  training_data = np.reshape(training_data, (number_training_samples, -1))\n",
        "  validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
        "  testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
        "\n",
        "  num_classes = np.max(training_labels) + 1\n",
        "  weights1 = np.ones((len(training_data[0]), num_classes))\n",
        "  weights2 = np.ones((len(training_data[1]), num_classes))\n",
        "  weights3 = np.ones((len(training_data[2]), num_classes))\n",
        "\n",
        "  weights_list = [weights1, weights2, weights3]\n",
        "\n",
        "  with open('svm_weights.csv', mode='w', newline='') as csv_file:\n",
        "    result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
        "    learning_rate= 0.00000001\n",
        "    reg_param= 50000\n",
        "    iterations= 15000\n",
        "    batch_size=200\n",
        "\n",
        "    for n_weight in weights_list:\n",
        "            num_tests = 0\n",
        "            total_accuracy = 0\n",
        "            total_runtime = 0\n",
        "\n",
        "            for i in range(2):\n",
        "              start_time = time.time()\n",
        "              res_weights = train_model(training_data, training_labels, n_weight, learning_rate, reg_param, iterations,batch_size)\n",
        "              training_time = time.time() - startTime\n",
        "              \n",
        "              total_accuracy+= calculate_accuracy(training_data, training_labels, n_weight)\n",
        "              total_runtime+= nano_to_seconds(training_time)\n",
        "              num_tests += 1\n",
        "            \n",
        "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
        "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
        "            result_writer.writerow([n_weight, learning_rate, reg_param, iterations, batch_size, total_accuracy, avg_runtime])\n",
        "\n",
        "test_svm_weights()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28mpQxaC_YgJ"
      },
      "source": [
        "**Testing learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXxtdOoZ_fX6"
      },
      "source": [
        "def test_svm_learning_rate():\n",
        "\n",
        "  learning_rates = [0.00001, 0.000001, 0.00000001]\n",
        "  training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess1(0.8)\n",
        "  num_classes = np.max(training_labels) + 1\n",
        "  weights = np.ones((len(training_data[0]), num_classes))\n",
        "\n",
        "  with open('svm_learning_rates.csv', mode='w', newline='') as csv_file:\n",
        "    \n",
        "    result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
        "    reg_param= 50000\n",
        "    iterations= 15000\n",
        "    batch_size=200\n",
        "\n",
        "    for n_learning_rate in learning_rates:\n",
        "            num_tests = 0\n",
        "            total_accuracy = 0\n",
        "            total_runtime = 0\n",
        "\n",
        "            for i in range(2):\n",
        "              start_time = time.time()\n",
        "              res_weights = train_model(training_data, training_labels, weights, n_learning_rate, reg_param, iterations,batch_size)\n",
        "              total_accuracy+=calculate_accuracy(training_data, training_labels, weights)\n",
        "              training_time = time.time() - startTime\n",
        "              print(calculate_accuracy(testing_data, testing_labels, weights))\n",
        "              total_runtime+= nano_to_seconds(training_time)\n",
        "              num_tests += 1\n",
        "            \n",
        "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
        "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
        "            result_writer.writerow([weights, n_learning_rate, reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
        "\n",
        "test_svm_learning_rate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYG50PBA_iZp"
      },
      "source": [
        "**Testing regularisation parameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3JmLDSy_oOr"
      },
      "source": [
        "def test_svm_regularisation_param():\n",
        "\n",
        "  reg_params = [1000, 5000, 10000]\n",
        "  training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess1(0.8)\n",
        "  num_classes = np.max(training_labels) + 1\n",
        "  weights = np.ones((len(training_data[0]), num_classes))\n",
        "\n",
        "  with open('svm_reg_param.csv', mode='w', newline='') as csv_file:\n",
        "    \n",
        "    result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
        "    learning_rate= 0.00001\n",
        "    iterations= 15000\n",
        "    batch_size=200\n",
        "\n",
        "    for n_reg_param in reg_params:\n",
        "            num_tests = 0\n",
        "            total_accuracy = 0\n",
        "            total_runtime = 0\n",
        "\n",
        "            for i in range(2):\n",
        "              start_time = time.time()\n",
        "              res_weights = train_model(training_data, training_labels, weights, learning_rate, n_reg_param, iterations,batch_size)\n",
        "              training_time = time.time() - startTime\n",
        "              total_accuracy+=calculate_accuracy(training_data, training_labels, weights)\n",
        "              total_runtime+= nano_to_seconds(training_time)\n",
        "              num_tests += 1\n",
        "            \n",
        "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
        "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
        "            result_writer.writerow([weights, learning_rate, n_reg_param, iterations, batch_size, avg_accuracy, avg_runtime])\n",
        "\n",
        "test_svm_regularisation_param()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQJOjDB_rUv"
      },
      "source": [
        "**Testing iterations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK1X-QgH_vz2"
      },
      "source": [
        "def test_svm_iterations():\n",
        "  \n",
        "  iterations = [15000, 20000, 35000]\n",
        "  training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
        "  num_classes = np.max(training_labels) + 1\n",
        "  weights = np.ones((len(training_data[0]), num_classes))\n",
        "\n",
        "  with open('svm_reg_param.csv', mode='w', newline='') as csv_file:\n",
        "    \n",
        "    result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
        "    learning_rate= 0.00001\n",
        "    reg_param= 5000\n",
        "    batch_size=200\n",
        "\n",
        "    for n_iteration in iterations:\n",
        "            num_tests = 0\n",
        "            total_accuracy = 0\n",
        "            total_runtime = 0\n",
        "\n",
        "            for i in range(2):\n",
        "              start_time = time.time()\n",
        "              res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, n_iteration,batch_size)\n",
        "              training_time = time.time() - startTime\n",
        "              total_accuracy+=calculate_accuracy(training_data, training_labels, weights)\n",
        "              total_runtime+= nano_to_seconds(training_time)\n",
        "              num_tests += 1\n",
        "            \n",
        "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
        "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
        "            result_writer.writerow([weights, learning_rate, reg_param, n_iteration, batch_size, avg_accuracy, avg_runtime])\n",
        "\n",
        "test_svm_iterations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DyixbQI_4lL"
      },
      "source": [
        "**Testing batch size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SdQhpr5_7Qu"
      },
      "source": [
        "def test_svm_batch_size():\n",
        "  \n",
        "  batch_sizes = [1000, 5000, 10000]\n",
        "  training_data, training_labels, testing_data, testing_labels, validation_data, validation_labels = load_in_dataset_and_preprocess(0.8)\n",
        "  num_classes = np.max(training_labels) + 1\n",
        "  weights = np.ones((len(training_data[0]), num_classes))\n",
        "\n",
        "  with open('svm_reg_param.csv', mode='w', newline='') as csv_file:\n",
        "    \n",
        "    result_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    result_writer.writerow(['Weights', 'Learning Rate', 'Regularisation Parameters', 'Iterations', 'Batch Size', 'Average Accuracy', 'Average Runtime'])\n",
        "    learning_rate= 0.00001\n",
        "    iterations= 15000\n",
        "    reg_param=5000\n",
        "\n",
        "    for n_batch_size in batch_sizes:\n",
        "            num_tests = 0\n",
        "            total_accuracy = 0\n",
        "            total_runtime = 0\n",
        "\n",
        "            for i in range(2):\n",
        "              start_time = time.time()\n",
        "              res_weights = train_model(training_data, training_labels, weights, learning_rate, reg_param, iterations,n_batch_size)\n",
        "              training_time = time.time() - startTime\n",
        "              total_accuracy+=calculate_accuracy(training_data, training_labels, weights)\n",
        "              total_runtime+= nano_to_seconds(training_time)\n",
        "              num_tests += 1\n",
        "            \n",
        "            avg_accuracy = np.round(total_accuracy / float(num_tests), 5)\n",
        "            avg_runtime = np.round(total_runtime / float(num_tests), 5)\n",
        "            result_writer.writerow([weights, learning_rate, reg_param, iterations, n_batch_size, avg_accuracy, avg_runtime])\n",
        "\n",
        "test_svm_batch_size()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}