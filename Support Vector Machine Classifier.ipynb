{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n",
      "(49000, 3072)\n",
      "Training time: 3.102876901626587\n",
      "Training acc:   22.33061224489796%\n",
      "Validating acc: 21.7%\n",
      "Testing acc:    22.03%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "(training_data, training_labels), (testing_data, testing_labels) = (cifar100.load_data(\"coarse\"))\n",
    "\n",
    "print(training_data.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_data.shape)\n",
    "print(testing_labels.shape)\n",
    "\n",
    "\n",
    "training_data = training_data.reshape(50000, 3072)\n",
    "testing_data = testing_data.reshape(10000, 3072)\n",
    "\n",
    "def centre_data(train, validation, test):\n",
    "    \n",
    "    # calculate the means for each attribute of the training data\n",
    "    column_means = np.mean(train, axis=0) \n",
    "    \n",
    "    # centre training data by subtracting training data attribute means\n",
    "    for i in range(len(train)):\n",
    "        train[i] = train[i] - column_means\n",
    "    \n",
    "    # centre testing data by subtracting training data attribute means\n",
    "    for x in range(len(test)):\n",
    "        test[x] = test[x] - column_means\n",
    "    \n",
    "    for x in range(len(validation)):\n",
    "        validation[x] = validation[x] - column_means\n",
    "        \n",
    "    return train, validation, test\n",
    "\n",
    "def PCA(variance_target, training_data, validation_data, testing_data):\n",
    "\n",
    "    U, sigma, Vt = np.linalg.svd(training_data, full_matrices=False)\n",
    "    \n",
    "    sum_square_singular = np.sum(sigma**2)\n",
    "    \n",
    "    ratios = sigma**2/sum_square_singular\n",
    "    \n",
    "                \n",
    "    n_components = 0\n",
    "    explained_variance = 0\n",
    "    \n",
    "    # determine how many principle components must be retained to maintain the target level of explained variance\n",
    "    for i in range(len(ratios)):\n",
    "        if explained_variance >= variance_target:\n",
    "            break\n",
    "        else: \n",
    "            n_components += 1\n",
    "            explained_variance += ratios[i]\n",
    "    \n",
    "    return training_data.dot(Vt.T[:, :n_components]), testing_data.dot(Vt.T[:, :n_components]), validation_data.dot(Vt.T[:, :n_components])\n",
    "\n",
    "validation_data = training_data[49000:, :].astype(np.float)\n",
    "validation_labels = np.squeeze(training_labels[49000:, :])\n",
    "training_data = training_data[:49000, :].astype(np.float)\n",
    "training_labels = np.squeeze(training_labels[:49000, :])\n",
    "testing_labels = np.squeeze(testing_labels)\n",
    "testing_data = testing_data.astype(np.float)\n",
    "\n",
    "print(training_data.shape)\n",
    "\n",
    "# Centre data\n",
    "training_data, validation_data, testing_data = centre_data(training_data, validation_data, testing_data)\n",
    "\n",
    "# Apply PCA\n",
    "training_data, testing_data, validation_data = PCA(0.8, training_data, validation_data, testing_data)\n",
    "\n",
    "number_training_samples = len(training_data)\n",
    "number_validation_samples = len(validation_data)\n",
    "number_testing_samples = len(testing_data)\n",
    "\n",
    "# Reshape data from channel to rows\n",
    "training_data = np.reshape(training_data, (number_training_samples, -1))\n",
    "validation_data = np.reshape(validation_data, (number_validation_samples, -1))\n",
    "testing_data = np.reshape(testing_data, (number_testing_samples, -1))\n",
    "\n",
    "# Add bias dimension columns\n",
    "training_data = np.hstack([training_data, np.ones((number_training_samples, 1))])\n",
    "validation_data = np.hstack([validation_data, np.ones((number_validation_samples, 1))])\n",
    "testing_data = np.hstack([testing_data, np.ones((number_testing_samples, 1))])\n",
    "\n",
    "num_classes = np.max(training_labels) + 1\n",
    "\n",
    "weights = np.ones((len(training_data[0]), num_classes))\n",
    "\n",
    "# calculate gradient, use L2 regularisation \n",
    "def calculate_gradient(training_data, training_labels, regularisation, weights):\n",
    "    \n",
    "    num_training_samples, num_training_features = training_data.shape\n",
    "    \n",
    "    gradient = np.zeros(num_training_features, num_classes)\n",
    "    \n",
    "    # w^Tx\n",
    "    linear_output = np.dot(training_data, weights)\n",
    "\n",
    "    #linear output with labels \n",
    "    linear_output_yi = linear_output[np.arange(num_training_samples),training_labels]\n",
    "\n",
    "    # distance of point from separating hyperplane?\n",
    "    # np.newaxis here makes it a column vector \n",
    "    # calculate distance?\n",
    "    \n",
    "    # distances = y * (np.dot(X, w)) - 1\n",
    "    delta = linear_output - linear_output_yi[:,np.newaxis] + 1\n",
    "    \n",
    "    ds = np.zeros(delta.shape)\n",
    "    \n",
    "    # makes all the places where delta > 0, 1 else 0\n",
    "    # With lagrange multiplier considered, if the sample is on the support vector: ð›¼ = 1\n",
    "    # else: ð›¼ = 0\n",
    "    ds = np.where(delta > 0, 1, 0)\n",
    "    \n",
    "    # calculate the sum of each row \n",
    "    sum_of_each_row = np.sum(ds, axis=1)\n",
    "    \n",
    "    \n",
    "    ds[np.arange(num_training_samples), training_labels] = - sum_of_each_row\n",
    "\n",
    "    gradient = (1/num_training_samples) * np.dot((training_data.T), ds)\n",
    "    \n",
    "    # controls the influence of each individual support vector on the objective function. \n",
    "    # Greater C decreases the effect of |w|Â²/2, and results in the narrower margin\n",
    "    gradient = gradient + (2* regularisation * weights)\n",
    "    \n",
    "    return gradient \n",
    "\n",
    "# train model using stochastic gradient descent \n",
    "def train_model(training_data, training_labels, weights, learning_rate, regularisation, iterations):\n",
    "    \n",
    "    # number of examples in each batch\n",
    "    batch_size = 200\n",
    "    \n",
    "    num_training_samples = len(training_data)\n",
    "    \n",
    "    weights = weights\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "        # create batch\n",
    "        batch = np.random.choice(num_training_samples, batch_size)\n",
    "\n",
    "        gradient = calculate_gradient(training_data[batch], training_labels[batch], regularisation, weights)\n",
    "        \n",
    "        weights = weights - learning_rate * gradient\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# calculate accuracy of model \n",
    "def calculate_accuracy (data, labels, weights):\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    prediction = np.zeros(len(data))\n",
    "    \n",
    "    #w^Tx\n",
    "    linear_output = np.dot(data, weights)\n",
    "    \n",
    "    # returns the indices of the maximum values along an axis, ie. in this case will return the \n",
    "    # column index corresponding to the greatest index of each row\n",
    "    prediction = np.argmax(linear_output, axis=1)\n",
    "    \n",
    "    # count the number of predictions that are correct \n",
    "    total_correct_predictions = (prediction == labels).sum()\n",
    "    \n",
    "    num_data_points = len(data)\n",
    "    \n",
    "    accuracy = (total_correct_predictions/num_data_points)*100\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "startTime = time.time()\n",
    "weights = train_model(training_data, training_labels, weights, 0.00000001, 50000, 15000)\n",
    "print ('Training time: {0}'.format(time.time() - startTime))\n",
    "print ('Training acc:   {0}%'.format(calculate_accuracy(training_data, training_labels, weights)))\n",
    "print ('Validating acc: {0}%'.format(calculate_accuracy(validation_data, validation_labels, weights)))\n",
    "print ('Testing acc:    {0}%'.format(calculate_accuracy(testing_data, testing_labels, weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
